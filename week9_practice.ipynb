{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week9_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudht/NLP/blob/master/week9_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9",
        "colab_type": "text"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-lNQfT1D1v",
        "colab_type": "code",
        "outputId": "cfceb493-e44f-4b49-d6cd-e060a8e34107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWZLECjoJkn",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델</h2>\n",
        "\n",
        "![실습 그림](http://nlp.kangwon.ac.kr/~nlpdemo/morpheme.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5861idd25QB",
        "colab_type": "code",
        "outputId": "b43f858b-4de4-4434-e095-1d78db367d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "class RNN(object):\n",
        "  \n",
        "  def __init__(self, flags):\n",
        "    self.encoder_vocab_size = flags[\"encoder_vocab_size\"]    # 음절 개수\n",
        "    self.label_vocab_size = flags[\"label_vocab_size\"]\n",
        "    self.embedding_size = flags[\"embedding_size\"]            # 음절 임베딩 사이즈\n",
        "    self.hidden_size = flags[\"hidden_size\"]                  # GRU 히든 사이즈\n",
        "\n",
        "    self.max_length = flags[\"max_length\"]                    # 음절 최대 길이, 짧게 설정하면 긴 음절에 대하여 처리하지 못함\n",
        "    self.keep_prob = flags[\"keep_prob\"]                      # 노드를 보전할 확률, 드랍아웃을 하이 않을 확률\n",
        "    self.learning_rate = flags[\"learning_rate\"]              # 학습률\n",
        "    self.mode = flags[\"mode\"]                                # 학습 or 평가 상태\n",
        "\n",
        "    self._input_init()\n",
        "    self._embedding_init()\n",
        "    self._rnn_init()\n",
        "    self._train_init()\n",
        "    self._predict_init()\n",
        "        \n",
        "  # 입력 데이터, 입력 데이터의 길이, 정답 데이터, keep_prob 값을 담을 tensor 선언  \n",
        "  def _input_init(self):\n",
        "    # 입력 데이터\n",
        "    self.inputs = tf.placeholder(tf.int32, [None, self.max_length], name=\"inputs\") # placeholder 나중에 어떠한 것이 들어올 공간 할당\n",
        "    \n",
        "    # 입력 데이터의 길이\n",
        "    self.inputs_length = tf.placeholder(tf.int32, [None], name=\"inputs_length\") # 최대 공간보다 작은 데이터의 경우 의미없는 작업을 하지 않기위해 필요\n",
        "\n",
        "    # 정답 데이터\n",
        "    self.targets = tf.placeholder(tf.int32, [None, self.max_length], name=\"targets\")\n",
        "    # loss를 구할 때 각 sequence의 가중치를 설정 (학습할 때 사용)\n",
        "    self.target_weights = tf.sequence_mask(lengths=self.inputs_length, maxlen=self.max_length, dtype=tf.float32, name='weight')\n",
        "    # 노드를 보전할 확률 (실수)\n",
        "    self.keep_prob = tf.placeholder(tf.float32, [], \"keep_prob\")\n",
        "\n",
        "  def _embedding_init(self):\n",
        "    # 일종의 소프트맥스? 일반화?\n",
        "    # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 tensor\n",
        "    # 기존에 사전학습 된 음절 임베딩을 사용할 수도 있고 랜덤으로 초기화 한 후,\n",
        "    # 모델 학습 과정 중에 같이 학습 시키는 것도 가능\n",
        "    # 예제 코드는 랜덤으로 초기화 한 후 같이 학습하도록 설정\n",
        "    # get_variable -> tf에서 변수 생성\n",
        "    self.encoder_embeddings = tf.get_variable(\"encoder_embedding\",\n",
        "                                             shape = [self.encoder_vocab_size, self.embedding_size],\n",
        "                                             dtype = tf.float32, trainable=True,\n",
        "                                             initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "\n",
        "  def _rnn_init(self):\n",
        "    with tf.name_scope(\"rnn_layer\"):\n",
        "      # rnn에서 사용할 cell 설정\n",
        "      # 예제 코드에서는 bidirectional lstm를 사용하기 때문에 정방향 cell과 역방향 cell 두개를 사용 (fw: forward, bw: backward)\n",
        "      fw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      bw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      \n",
        "      # 임베딩 tensor를 이용하여 입력데이터의 각 음절 index를 대응하는 임베딩 벡터로 치환\n",
        "      # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "      self.lookup_inputs = tf.nn.embedding_lookup(self.encoder_embeddings, self.inputs)\n",
        "\n",
        "      # tf.nn.bidirectional_dynamic_rnn 라이브러리를 사용하여 rnn layer의 출력을 구한다\n",
        "      # outputs : 각 step의 rnn 출력값, state : 마지막 hidden state 값\n",
        "      (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, self.lookup_inputs,\n",
        "                                                                                       sequence_length=self.inputs_length,\n",
        "                                                                                       dtype=tf.float32,\n",
        "                                                                                       time_major=False)\n",
        "\n",
        "      # 예제 코드에서는 각 step의 출력값을 사용한다.\n",
        "      # 양방향으로 생성된 rnn 결과를 연결하여 하나의 벡터로 표현\n",
        "      # (batch_size, max_length, hidden_size)*2 -> (batch_size, max_length, hidden_size*2)\n",
        "      outputs = tf.concat([fw_outputs, bw_outputs], axis=2)\n",
        "      \n",
        "      # fully_connected layer를 통하여 출력 크기를 label_vocab_size에 맞춰줌\n",
        "      # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, label_vocab_size)\n",
        "      self.outputs = tf.layers.dense(inputs=outputs,\n",
        "                                     units=self.label_vocab_size,\n",
        "                                     activation=None,\n",
        "                                     use_bias=False,\n",
        "                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "    \n",
        "    \n",
        "  def _predict_init(self):\n",
        "    # 각 라벨의 분포 값들을 softmax 함수를 이용하여 0~1사이의 값으로 변경\n",
        "    self.predict_op = tf.nn.softmax(logits=self.outputs, axis=-1)\n",
        "\n",
        "  def _train_init(self):\n",
        "    if self.mode == \"train\":\n",
        "      with tf.variable_scope(\"train_layer\"):\n",
        "        # 모델의 출력인 self.outputs 와 self.targets를 비교하여 loss 계산\n",
        "        self.loss = tf.contrib.seq2seq.sequence_loss(self.outputs, self.targets, self.target_weights) # sequence_loss -> CE(cross entropy) 내장, target_weights -> 쓸데없이 작업하지 않아도 되는 부분 알려줌\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate) # AdamOptimizer는 learning_rate가 굉장히 낮아야 한다.\n",
        "        self.grads = self.optimizer.compute_gradients(self.loss)\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwrnE5li9bmF",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 읽고 전처리 하기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path)</b>\n",
        "  \"train_datas.txt\", \"test_datas.txt\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  데이터 예시)\n",
        "    말 은 [SP] 옳 은 [SP] 말 이 다 . \\t B_NNG B_JX [SP] B_VA B_ETM [SP] B_NNG B_VCP B_EF B_SF\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    \n",
        "    출력 예시)\n",
        "      [\"말\", \"은\", \"[SP]\", \"옳\", \"은\", \"[SP]\", \"말\", \"이\", \"다\"], 10, [\"B_NNG\", \"B_JX\", \"[SP]\", \"B_VA\", \"B_ETM\", \"[SP]\", \"B_NNG\", \"B_VCP\", \"B_EF\", \"B_SF\"]\n",
        "      \n",
        "<b>2. read_vocab_file(flags)</b>\n",
        "  \"eumjeol_vocab.txt\", \"label_vocab.txt\" 파일을 읽고 음절과 라벨을 indexing하기 위한 딕셔너리를 생성\n",
        "   \n",
        "  read_vocab_file(flags)\n",
        "  args\n",
        "    flags : hyperparameter들을 담고 있는 딕셔너리\n",
        "  return\n",
        "    eumjeol2idx, idx2eumjeol, label2idx, idx2label 딕셔너리\n",
        "    \n",
        "    eumjeol2idx : 음절을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환하기 위한 딕셔너리\n",
        "    label2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환하기 위한 딕셔너리\n",
        "    \n",
        "<b>3. prepro(datas, eumjeol2idx, label2idx)</b>\n",
        "  입력 데이터의 각 음절과 라벨을 indexing, 음절 sequence의 길이를 고정된 길이로 변환\n",
        "   \n",
        "  prepro(datas, eumjeol2idx, label2idx)\n",
        "  args\n",
        "    file_path : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "  return\n",
        "    입력 데이터의 각 음절을 indexing하고 라벨을 one_hot 인코딩한 리스트\n",
        "    \n",
        "  전처리 예시)\n",
        "    [\"말\", \"은\", \"[SP]\", \"옳\", \"은\", \"[SP]\", \"말\", \"이\", \"다\", \".\"], 15, [\"B_NNG\", \"B_JX\", \"[SP]\", \"B_VA\", \"B_ETM\", \"[SP]\", \"B_NNG\", \"B_VCP\", \"B_EF\", \"B_SF\"]\n",
        "    \n",
        "    사전 설정한 문장의 최대 길이를 15이라고 가정\n",
        "    \n",
        "    [\"말\", \"은\", \"[SP]\", \"옳\", \"은\", \"[SP]\", \"말\", \"이\", \"다\"] -> [\"말\", \"은\", \"[SP]\", \"옳\", \"은\", \"[SP]\", \"말\", \"이\", \"다\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\"]\n",
        "    [\"말\", \"은\", \"[SP]\", \"옳\", \"은\", \"[SP]\", \"말\", \"이\", \"다\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\"] -> [ 23, 2, 55, 65, 96, 12, 4, 11, 235, 5, 0, 0, 0, 0, 0 ]\n",
        "    \n",
        "    [\"B_NNG\", \"B_JX\", \"[SP]\", \"B_VA\", \"B_ETM\", \"[SP]\", \"B_NNG\", \"B_VCP\", \"B_EF\", \"B_SF\"] -> [\"B_NNG\", \"B_JX\", \"[SP]\", \"B_VA\", \"B_ETM\", \"[SP]\", \"B_NNG\", \"B_VCP\", \"B_EF\", \"B_SF\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\"]\n",
        "    [\"B_NNG\", \"B_JX\", \"[SP]\", \"B_VA\", \"B_ETM\", \"[SP]\", \"B_NNG\", \"B_VCP\", \"B_EF\", \"B_SF\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\"] -> [ 43, 21, 3, 4, 96, 12, 4, 111, 56, 9, 0, 0, 0, 0, 0 ]    \n",
        "\n",
        "    라벨의 경우 indexing 예시만 표기했지만 실제로는 one_hot 인코딩으로 변환\n",
        "    예를 들어 index 값이 3이고 전체 vocab 사이즈가 6인 경우\n",
        "    3 -> [0, 0, 0, 1, 0, 0]\n",
        " </pre>\n",
        "        \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hopD20S9VJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 데이터를 읽고 음절 sequence와 라벨 sequence를 분리\n",
        "# 음절 sequence의 길이를 구하고 음절 sequence, 음절 sequence의 길이, 라벨 sequence를 datas 리스트에 저장\n",
        "def read_file(file_path):\n",
        "  with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "    lines = inFile.readlines()\n",
        "  datas = []\n",
        "  for line in lines:\n",
        "    # 입력 문장을 \\t으로 분리\n",
        "    pieces = line.strip().split(\"\\t\")\n",
        "    eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n",
        "    eumjeol_length = len(eumjeol_sequence)\n",
        "\n",
        "    datas.append((eumjeol_sequence, eumjeol_length, label_sequence))\n",
        "\n",
        "  return datas\n",
        "  \n",
        "def read_vocab_file(flags):\n",
        "  eumjeol2idx, idx2eumjeol = {\"<PAD>\":0}, {0:\"<PAD>\"}\n",
        "  label2idx, idx2label = {\"<PAD>\":0}, {0:\"<PAD>\"}\n",
        "  \n",
        "  with open(flags[\"eumjeol_vocab_data_path\"], \"r\", encoding=\"utf8\") as inFile:\n",
        "    eumjeols = inFile.readlines()\n",
        "  with open(flags[\"label_vocab_data_path\"], \"r\", encoding=\"utf8\") as inFile:\n",
        "    labels = inFile.readlines()\n",
        "\n",
        "  for eumjeol in eumjeols:\n",
        "    eumjeol = eumjeol.strip()\n",
        "\n",
        "    if(eumjeol not in eumjeol2idx):\n",
        "      eumjeol2idx[eumjeol] = len(eumjeol2idx)\n",
        "\n",
        "    if(eumjeol2idx[eumjeol] not in idx2eumjeol):\n",
        "      idx2eumjeol[eumjeol2idx[eumjeol]] = eumjeol\n",
        "\n",
        "  for label in labels:\n",
        "    label = label.strip()\n",
        "\n",
        "    if(label not in label2idx):\n",
        "      label2idx[label] = len(label2idx)\n",
        "\n",
        "    if(label2idx[label] not in idx2label):\n",
        "      idx2label[label2idx[label]] = label\n",
        "      \n",
        "  return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n",
        "        \n",
        "# 입력 데이터 전처리 \n",
        "def prepro(datas, eumjeol2idx, label2idx, max_length):\n",
        "  \n",
        "  preprocessed_datas = []\n",
        "  for eumjeol_sequence, eumjeol_length, label_sequence in datas:\n",
        "    \n",
        "    # max_length의 길이를 가지는 numpy array 생성하고 0으로 초기화\n",
        "    indexing_eumjeol_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    indexing_label_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    \n",
        "    # 각 음절과 라벨 index를 numpy array에 입력\n",
        "    for index in range(eumjeol_length):\n",
        "      indexing_eumjeol_sequence[index] = eumjeol2idx[eumjeol_sequence[index]]\n",
        "      indexing_label_sequence[index] = label2idx[label_sequence[index]]\n",
        "      \n",
        "    preprocessed_datas.append((indexing_eumjeol_sequence, eumjeol_length, indexing_label_sequence))\n",
        "\n",
        "  return preprocessed_datas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTgxrex_qW--",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 batch 단위로 나누기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>get_batch(datas, batch_size)</b>\n",
        "  전체 데이터를 batch 단위로 나누어 주기 위한 함수\n",
        "  \n",
        "  get_batch(datas, batch_size)\n",
        "  args\n",
        "    datas : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    batch_size : 한번에 학습할 데이터의 개수\n",
        "  return\n",
        "    batch 단위로 나뉘어진 데이터 리스트\n",
        "    \n",
        "  예시) \n",
        "    batch_size = 3 인 경우\n",
        "  \n",
        "    total_datas = [ (음절 sequence1, sequence1의 길이, 라벨 sequence1), ... , ((음절 sequence100, sequence100의 길이, 라벨 sequence100)) ]\n",
        "    \n",
        "    batches = [\n",
        "    [ [ 음절 sequence1, 음절 sequence2, 음절 sequence3 ], \n",
        "      [ sequence1의 길이, sequence2의 길이, sequence3의 길이 ],\n",
        "      [ 라벨 sequence1, 라벨 sequence2, 라벨 sequence3 ]\n",
        "    ],\n",
        "    \n",
        "    [ [ 음절 sequence4, 음절 sequence5, 음절 sequence6 ], \n",
        "      [ sequence4의 길이, sequence5의 길이, sequence6의 길이 ],\n",
        "      [ 라벨 sequence4, 라벨 sequence5, 라벨 sequence6 ]\n",
        "    ],\n",
        "    \n",
        "    ...\n",
        "    \n",
        "    ]\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG3GWKrAqQzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터를 batch 단위로 분할하여 저장\n",
        "def get_batch(datas, batch_size):\n",
        "  # batches : batch 단위로 저장할 리스트\n",
        "  # inputs : 각 batch 단위 별 음절 sequence를 저장할 리스트\n",
        "  # inputs_length : 각 batch 단위 별 음절 sequence 길이를 저장할 리스트\n",
        "  # targets : 각 batch 단위 별 라벨 sequence를 저장할 리스트\n",
        "  batches, inputs, inputs_length, targets = [], [], [], []\n",
        "    \n",
        "  for indexing_eumjeol_sequence, eumjeol_length, indexing_label_sequence in datas:\n",
        "    inputs.append(indexing_eumjeol_sequence)\n",
        "    inputs_length.append(eumjeol_length)\n",
        "    targets.append(indexing_label_sequence)\n",
        "    \n",
        "    if(len(inputs) == batch_size):\n",
        "      batches.append((inputs, inputs_length, targets))\n",
        "      inputs, inputs_length, targets = [], [], []\n",
        "\n",
        "  return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4uOGj7vwFFY",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(flags) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 학습 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>3. epoch를 돌때마다 학습 데이터 셔플</b>\n",
        "\n",
        "<b>3. batch 단위로 학습을 수행</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPLgRpO9SAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags)\n",
        "  # 학습 데이터 읽기\n",
        "  train_datas = read_file(flags[\"train_data_path\"])\n",
        "  # 학습 데이터 전처리\n",
        "  preprocessed_train_datas = prepro(train_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  \n",
        "  # tensorflow session 옵션 설정\n",
        "  # allow_soft_placement=True : 어떤 device를 사용하여 연산할지 명시하지 않은 경우 자동으로 존재하는 디바이스 중에서 하나를 선택\n",
        "  # gpu_options=tf.GPUOptions(allow_growth=True) : 연산 실행 과정에서 필요한만큼의 gpu 메모리만 사용\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    for epoch in tqdm(range(flags[\"epoch\"])):\n",
        "      # 학습 데이터 셔플\n",
        "      np.random.shuffle(preprocessed_train_datas)\n",
        "      # 학습 데이터를 batch 단위로 분할하여 저장\n",
        "      batches = get_batch(preprocessed_train_datas, flags[\"batch_size\"])\n",
        "\n",
        "      losses = []\n",
        "      # batch 단위로 학습을 진행하며 각 batch 별 loss를 구한다\n",
        "      # batch 별 loss들의 평균을 구하여 이를 전체 데이터에 대한 loss로 사용\n",
        "      for inputs, inputs_length, targets in batches:\n",
        "        loss, train_op = sess.run([model.loss, model.train_op],\n",
        "                                  feed_dict={ model.inputs:inputs, \n",
        "                                             model.inputs_length:inputs_length, \n",
        "                                             model.targets:targets, \n",
        "                                             model.keep_prob:flags[\"keep_prob\"] }\n",
        "                                 )\n",
        "\n",
        "        losses.append(loss)\n",
        "        \n",
        "      # 학습한 모델 파일 저장\n",
        "      filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n",
        "      saver.save(sess, filename)\n",
        "\n",
        "      print(\"\\tEpoch : {}, Average_Loss : {}\".format(epoch+1, np.mean(losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wzroGrw2xm",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(flags) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 평가 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>5. tf.train.Saver() 객체를 사용하여 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴</b>\n",
        "\n",
        "<b>6. make_sentence(inputs, predict, correct, idx2eumjeol, idx2label) 함수를 이용하여 정답과 모델 출력 비교</b>\n",
        "  \n",
        "  make_sentence(inputs, predict, correct, idx2eumjeol, idx2label)\n",
        "  args\n",
        "    inputs : 음절 sequence\n",
        "    predict : 모델 출력 라벨 sequence\n",
        "    correct : 정답 라벨 sequence\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환해주는 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환해주는 딕셔너리\n",
        "  return\n",
        "    정답 문장과 모델 출력 문장\n",
        "\n",
        "  예시)\n",
        "    inputs = ['아', '직', '도', '[SP]', '울', '지', '[SP]', '않', '소', '?',]\n",
        "    predict = ['B_NNG', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VX', 'B_EF', 'B_SF']\n",
        "    correct = ['B_MAG', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VX', 'B_EF', 'B_SF']\n",
        "    \n",
        "    정답 문장 : 아직/MAG 도/JX [SP] 울/VV 지/EC [SP] 않/VX 소/EF ?/SF\n",
        "    모델 출력 문장 : 아직/NNG 도/JX [SP] 울/VV 지/EC [SP] 않/VX 소/EF ?/SF\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPLfTilw9MVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 모델 출력 라벨 sequence와 정답 라벨 sequence를 기반으로\n",
        "# 모델 출력 문장과 정답 문장 출력\n",
        "def make_sentence(inputs, predict, correct, idx2eumjeol, idx2label):\n",
        "  # 빈 문자열 생성\n",
        "  predict_sentence, correct_sentence = \"\", \"\"\n",
        "  \n",
        "  predict_pumsa, correct_pumsa = None, None\n",
        "  for index in range(len(predict)):\n",
        "    eumjeol = idx2eumjeol[inputs[index]]\n",
        "    correct_label = idx2label[correct[index]]\n",
        "    predict_label = idx2label[predict[index]]\n",
        "\n",
        "    if(eumjeol == \"<SP>\"):\n",
        "      correct_sentence += (\"/\" + correct_pumsa + \" <SP> \")\n",
        "      predict_sentence += (\"/\" + predict_pumsa + \" <SP> \")\n",
        "      predict_pumsa, correct_pumsa = None, None\n",
        "    else:\n",
        "      if(correct_label[0] == \"B\"):\n",
        "        if(correct_pumsa != None):\n",
        "          correct_sentence += (\"/\" + correct_pumsa + \" \")\n",
        "        correct_pumsa = correct_label.split(\"_\")[-1]\n",
        "\n",
        "      if(predict_label[0] == \"B\"):\n",
        "        if(predict_pumsa != None):\n",
        "          predict_sentence += (\"/\" + predict_pumsa + \" \")\n",
        "        predict_pumsa = predict_label.split(\"_\")[-1]\n",
        "      \n",
        "      # if correct_label[0] == \"I\":\n",
        "      #   result = correct_label.split(\"+\")\n",
        "      #   if len(result) > 1:\n",
        "      #     correct_pumsa += result[1]\n",
        "\n",
        "      predict_sentence += eumjeol\n",
        "      correct_sentence += eumjeol\n",
        "\n",
        "  correct_sentence += (\"/\" + correct_pumsa)\n",
        "  predict_sentence += (\"/\" + predict_pumsa)\n",
        "    \n",
        "  return predict_sentence, correct_sentence\n",
        "      \n",
        "\n",
        "def test(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags)\n",
        "  # 평가 데이터 읽기\n",
        "  test_datas = read_file(flags[\"test_data_path\"])\n",
        "  # 평가 데이터 전처리\n",
        "  preprocessed_test_datas = prepro(test_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  # tensorflow session 옵션 설정\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n",
        "    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n",
        "\n",
        "    # np.random.shuffle(preprocessed_test_datas)\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 분할하여 저장\n",
        "    batches = get_batch(preprocessed_test_datas, flags[\"batch_size\"])\n",
        "\n",
        "    # 전체 음절 수, 정답을 맞춘 음절 수\n",
        "    total_count, correct_count = 0, 0\n",
        "    for inputs, inputs_length, targets in tqdm(batches[:10]):\n",
        "      predict_op = sess.run(model.predict_op,\n",
        "                            feed_dict={ model.inputs:inputs, \n",
        "                                       model.inputs_length:inputs_length, \n",
        "                                       model.keep_prob:flags[\"keep_prob\"] }\n",
        "                           )\n",
        "      \n",
        "      # 모델의 outputs에는 각 클래스에 대한 분포가 저장되어 있고\n",
        "      # np.argmax 함수를 통하여 가장 확률이 높은 클래스를 선택\n",
        "      # 예시) \n",
        "      #  predict_op = [0,1, 0.3, 0.2] (각각 0일 확률, 1일 확률, 2일 확률)\n",
        "      #  np.argmax(predict_op) = 1\n",
        "      predict, correct = np.argmax(predict_op[0], axis=-1), targets[0]\n",
        "      \n",
        "      # padding 처리해준 부분 제거\n",
        "      predict, correct = np.trim_zeros(predict), np.trim_zeros(correct)\n",
        "      \n",
        "      print([idx2eumjeol[temp] for temp in inputs[0]])\n",
        "      print([idx2label[temp] for temp in predict])\n",
        "      print([idx2label[temp] for temp in correct])\n",
        "\n",
        "      predict_sentence, correct_sentence = make_sentence(inputs[0], predict, correct, idx2eumjeol, idx2label)\n",
        "      print(\"\\n정답 : \" + correct_sentence)\n",
        "      print(\"출력 : \" + predict_sentence)\n",
        "      print()\n",
        "      \n",
        "      correct_count += np.sum(np.equal(predict, correct))\n",
        "      total_count += inputs_length[0]\n",
        "\n",
        "    print(\"Accuracy : \" + str(100.0*correct_count/total_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b7ddMCzt8_",
        "colab_type": "text"
      },
      "source": [
        "<h2>모델의 hyper parameter 설정, 학습 및 평가 실행</h2>\n",
        "\n",
        "<pre>\n",
        "root_dir : 코드와 데이터가 있는 디렉토리 경로\n",
        "save_dir : 학습한 모델 파일을 저장할 디렉토리 경로(디렉토리가 존재하지 않을 경우 자동으로 생성)\n",
        "\n",
        "<b>flags : hyper parameter를 저장할 딕셔너리</b>\n",
        "  flags.mode = 학습 또는 평가 설정(\"train\" or \"test\")\n",
        "  flags.save_dir = 학습한 모델 파일을 저장할 디렉토리 경로\n",
        "  flags.batch_size = 한번에 학습할 데이터의 개수\n",
        "  flags.epoch = 학습 횟수\n",
        "  flags.learning_rate = 학습률\n",
        "  flags.keep_prob = 노드를 보전할 확률\n",
        "  flags.max_length = 음절 sequence의 최대 길이\n",
        "  flags.embedding_size = 음절 임베딩 사이즈\n",
        "  flags.hidden_size = rnn cell의 히든 사이즈\n",
        "  flags.encoder_vocab_size = 음절 어휘 딕셔너리의 사이즈\n",
        "  flags.label_vocab_size = 라벨 딕셔너리의 사이즈\n",
        "  flags.train_data_path = 학습데이터 파일 경로\n",
        "  flags.test_data_path = 평가데이터 파일 경로\n",
        "  flags.vocab_data_path = 음절 어휘 파일 경로\n",
        "\n",
        "<b>mode 별 hyper parameter 변경</b>\n",
        "  학습하는 경우 : mode를 \"train\"으로 설정, 나머지는 기본 설정 그대로 유지\n",
        "  평가하는 경우 : mode를 \"test\"로, batch_size는 1로, keep_prob은 1.0으로 변경\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQfWrdn33utZ",
        "colab_type": "code",
        "outputId": "26222afa-0644-4164-fd1a-a67aa7a1c8ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_dir = \"/gdrive/My Drive/colab/week9\"\n",
        "  save_dir = os.path.join(root_dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"test\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":1,\n",
        "           \"epoch\":20,\n",
        "           \"learning_rate\":0.0001,\n",
        "           \"keep_prob\":1.0,\n",
        "           \"max_length\":30,\n",
        "           \"embedding_size\":100,\n",
        "           \"hidden_size\":100,\n",
        "           \"encoder_vocab_size\":2065,\n",
        "           \"label_vocab_size\":248,\n",
        "           \"train_data_path\":os.path.join(root_dir, \"train_datas.txt\"),\n",
        "           \"test_data_path\":os.path.join(root_dir, \"test_datas.txt\"),\n",
        "           \"eumjeol_vocab_data_path\":os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n",
        "           \"label_vocab_data_path\":os.path.join(root_dir, \"label_vocab.txt\")\n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags)\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:00<00:01,  7.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Read from : /gdrive/My Drive/colab/week9/model/model_20.ckpt\n",
            "['무', '섭', '긴', '<SP>', '뭐', '가', '<SP>', '무', '섭', '소', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'I', '<SP>', 'B_NNG', 'B_JKS', '<SP>', 'B_NNG', 'I', 'I', 'B_SF']\n",
            "['B_VA', 'I', 'B_ETN+JX', '<SP>', 'B_NP', 'B_JKS', '<SP>', 'B_VA', 'I', 'B_EF', 'B_SF']\n",
            "\n",
            "정답 : 무섭/VA 긴/ETN+JX <SP> 뭐/NP 가/JKS <SP> 무섭/VA 소/EF ./SF\n",
            "출력 : 무섭긴/NNG <SP> 뭐/NNG 가/JKS <SP> 무섭소/NNG ./SF\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [00:00<00:00, 12.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['인', '저', '<SP>', '안', '<SP>', '가', '시', '는', '<SP>', '거', '쥬', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EF', 'B_SF']\n",
            "['B_MAG', 'I', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'B_EP', 'B_ETM', '<SP>', 'B_NNB', 'B_VCP+EF', 'B_SF']\n",
            "\n",
            "정답 : 인저/MAG <SP> 안/MAG <SP> 가/VV 시/EP 는/ETM <SP> 거/NNB 쥬/VCP+EF ?/SF\n",
            "출력 : 인저/NNG <SP> 안/MAG <SP> 가시/VV 는/JX <SP> 거/VV 쥬/EF ?/SF\n",
            "\n",
            "['인', '저', '<SP>', '아', '무', '디', '루', '두', '<SP>', '안', '<SP>', '가', '시', '는', '<SP>', '거', '쥬', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', '<SP>', 'B_NNG', 'I', 'I', 'I', 'I', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EF', 'B_SF']\n",
            "['B_MAG', 'I', '<SP>', 'B_MM', 'I', 'B_NNB', 'B_JKB', 'B_JX', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'B_EP', 'B_ETM', '<SP>', 'B_NNB', 'B_VCP+EF', 'B_SF']\n",
            "\n",
            "정답 : 인저/MAG <SP> 아무/MM 디/NNB 루/JKB 두/JX <SP> 안/MAG <SP> 가/VV 시/EP 는/ETM <SP> 거/NNB 쥬/VCP+EF ?/SF\n",
            "출력 : 인저/NNG <SP> 아무디루두/NNG <SP> 안/MAG <SP> 가시/VV 는/JX <SP> 거/VV 쥬/EF ?/SF\n",
            "\n",
            "['아', '직', '도', '<SP>', '울', '지', '<SP>', '않', '소', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VX', 'I', 'B_SF']\n",
            "['B_MAG', 'I', 'B_JX', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VX', 'B_EF', 'B_SF']\n",
            "\n",
            "정답 : 아직/MAG 도/JX <SP> 울/VV 지/EC <SP> 않/VX 소/EF ?/SF\n",
            "출력 : 아직/NNG 도/JX <SP> 울/VV 지/EC <SP> 않소/VX ?/SF\n",
            "\n",
            "['손', '길', '도', '<SP>', '언', '덕', '도', '<SP>', '함', '께', '<SP>', '경', '련', '했', '다', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'B_JX', '<SP>', 'B_NNG', 'I', 'B_JX', '<SP>', 'B_MAG', 'I', '<SP>', 'B_NNG', 'I', 'B_XSV+EP', 'B_EF', 'B_SF']\n",
            "['B_NNG', 'I', 'B_JX', '<SP>', 'B_NNG', 'I', 'B_JX', '<SP>', 'B_MAG', 'I', '<SP>', 'B_NNG', 'I', 'B_XSV+EP', 'B_EF', 'B_SF']\n",
            "\n",
            "정답 : 손길/NNG 도/JX <SP> 언덕/NNG 도/JX <SP> 함께/MAG <SP> 경련/NNG 했/XSV+EP 다/EF ./SF\n",
            "출력 : 손길/NNG 도/JX <SP> 언덕/NNG 도/JX <SP> 함께/MAG <SP> 경련/NNG 했/XSV+EP 다/EF ./SF\n",
            "\n",
            "['골', '방', '에', '<SP>', '아', ',', '<SP>', '안', '<SP>', '들', '어', '가', '셔', '두', '<SP>', '되', '넌', '규', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'B_JKB', '<SP>', 'B_MAG', 'B_SP', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'I', 'I', 'B_EC', 'I', '<SP>', 'B_VV', 'B_EF', 'I', 'B_SF']\n",
            "['B_NNG', 'I', 'B_JKB', '<SP>', 'B_IC', 'B_SP', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'I', 'I', 'I+EC', 'I', '<SP>', 'B_VV', 'B_EF', 'I', 'B_SF']\n",
            "\n",
            "정답 : 골방/NNG 에/JKB <SP> 아/IC ,/SP <SP> 안/MAG <SP> 들어가셔두/VV <SP> 되/VV 넌규/EF ?/SF\n",
            "출력 : 골방/NNG 에/JKB <SP> 아/MAG ,/SP <SP> 안/MAG <SP> 들어가/VV 셔두/EC <SP> 되/VV 넌규/EF ?/SF\n",
            "\n",
            "['뭐', '라', '구', '<SP>', '말', '씀', '<SP>', '점', '<SP>', '허', '세', '유', ',', '<SP>', '뭐', '라', '구', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_MAJ', 'I', 'I', '<SP>', 'B_NNG', 'I', '<SP>', 'B_NNG', '<SP>', 'B_NNG', 'I', 'I', 'B_SP', '<SP>', 'B_VA', 'B_EF', 'I', 'B_SF']\n",
            "['B_NP', 'B_VCP+B_NY', 'I_EC', '<SP>', 'B_NNG', 'I', '<SP>', 'B_MAG', '<SP>', 'B_VV', 'B_EP+EC', 'I', 'B_SP', '<SP>', 'B_NP', 'B_VCP+B_NY', 'I_EF', 'B_SF']\n",
            "\n",
            "정답 : 뭐/NP 라구/NY <SP> 말씀/NNG <SP> 점/MAG <SP> 허/VV 세유/EP+EC ,/SP <SP> 뭐/NP 라구/NY ./SF\n",
            "출력 : 뭐라구/MAJ <SP> 말씀/NNG <SP> 점/NNG <SP> 허세유/NNG ,/SP <SP> 뭐/VA 라구/EF ./SF\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [00:00<00:00, 14.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['머', '리', '밑', '으', '로', '<SP>', '받', '쳐', '준', '<SP>', '팔', '에', '<SP>', '힘', '을', '<SP>', '주', '어', '<SP>', '당', '겼', '다', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'I', 'B_JKB', 'I', '<SP>', 'B_VV', 'I', 'I', '<SP>', 'B_NNG', 'B_JKB', '<SP>', 'B_NNG', 'B_JKO', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VV', 'I+EP', 'B_EF', 'B_SF']\n",
            "['B_NNG', 'I', 'B_NNG', 'B_JKB', 'I', '<SP>', 'B_VV', 'I+EC', 'B_VX+ETM', '<SP>', 'B_NNG', 'B_JKB', '<SP>', 'B_NNG', 'B_JKO', '<SP>', 'B_VV', 'B_EC', '<SP>', 'B_VV', 'I+EP', 'B_EF', 'B_SF']\n",
            "\n",
            "정답 : 머리/NNG 밑/NNG 으로/JKB <SP> 받쳐/VV 준/VX+ETM <SP> 팔/NNG 에/JKB <SP> 힘/NNG 을/JKO <SP> 주/VV 어/EC <SP> 당겼/VV 다/EF ./SF\n",
            "출력 : 머리밑/NNG 으로/JKB <SP> 받쳐준/VV <SP> 팔/NNG 에/JKB <SP> 힘/NNG 을/JKO <SP> 주/VV 어/EC <SP> 당겼/VV 다/EF ./SF\n",
            "\n",
            "['싸', '아', '하', '니', '<SP>', '코', '끝', '에', '<SP>', '감', '기', '는', '<SP>', '동', '백', '기', '름', '<SP>', '냄', '새', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_VV', 'I', 'B_XSV', 'I', '<SP>', 'B_NNG', 'I', 'B_JKB', '<SP>', 'B_NNG', 'I', 'B_JX', '<SP>', 'B_NNG', 'I', 'I', 'I', '<SP>', 'B_NNG', 'I', 'B_SF']\n",
            "['B_VA', 'I', 'I', 'B_EC', '<SP>', 'B_NNG', 'I', 'B_JKB', '<SP>', 'B_VV', 'I', 'B_ETM', '<SP>', 'B_NNG', 'I', 'I', 'I', '<SP>', 'B_NNG', 'I', 'B_SF']\n",
            "\n",
            "정답 : 싸아하/VA 니/EC <SP> 코끝/NNG 에/JKB <SP> 감기/VV 는/ETM <SP> 동백기름/NNG <SP> 냄새/NNG ./SF\n",
            "출력 : 싸아/VV 하니/XSV <SP> 코끝/NNG 에/JKB <SP> 감기/NNG 는/JX <SP> 동백기름/NNG <SP> 냄새/NNG ./SF\n",
            "\n",
            "['구', '체', '적', '으', '로', '<SP>', '와', '닿', '는', '<SP>', '살', '냄', '새', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['B_NNG', 'I', 'B_XSN', 'B_JKB', 'I', '<SP>', 'B_VV', 'I', 'B_JX', '<SP>', 'B_NNG', 'I', 'I', 'B_SF']\n",
            "['B_NNG', 'I', 'B_XSN', 'B_JKB', 'I', '<SP>', 'B_VV+EC', 'B_VV', 'B_ETM', '<SP>', 'B_NNG', 'B_NNG', 'I', 'B_SF']\n",
            "\n",
            "정답 : 구체/NNG 적/XSN 으로/JKB <SP> 와/VV+EC 닿/VV 는/ETM <SP> 살/NNG 냄새/NNG ./SF\n",
            "출력 : 구체/NNG 적/XSN 으로/JKB <SP> 와닿/VV 는/JX <SP> 살냄새/NNG ./SF\n",
            "\n",
            "Accuracy : 72.8395061728395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}