{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudht/NLP/blob/master/week6_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9",
        "colab_type": "text"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-lNQfT1D1v",
        "colab_type": "code",
        "outputId": "7ba90dd4-24f1-4e97-b1a0-e96dbd08aae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5861idd25QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "class FNN(object):\n",
        "  \n",
        "  def __init__(self, flags):\n",
        "    self.learning_rate = flags[\"learning_rate\"]  # 학습률\n",
        "    self.num_classes = flags[\"num_classes\"]      # 분류할 클래스의 개수, 여기선 숫자 분류니 10개(0~9)\n",
        "    self.mode = flags[\"mode\"]                    # 학습 or 평가 상태\n",
        "\n",
        "    self._input_init()\n",
        "    self._fnn_init()\n",
        "    self._predict_init()\n",
        "    self._train_init()\n",
        "\n",
        "  # 여러 데이터 한번에 학습 -> batch (ex. batch size가 높으면 한번에 여러개 학습)\n",
        "  # batch로 들어온 크기의 평균만큼 학습하고, 이런 용도로 사용, batchsize가 너무 작으면 이리저리 왔다갔다함\n",
        "  # 입력 데이터, 정답 데이터, keep_prob 값을 담을 tensor 선언 (드랍아웃, 오버피팅 방지)\n",
        "  def _input_init(self):\n",
        "    # 입력 데이터\n",
        "    self.inputs = tf.placeholder(tf.float32, [None, 28, 28], name=\"inputs\")   # Nome은 batchsize의 위치, batch, 28 * 28\n",
        "    \n",
        "    \n",
        "    # 정답 데이터\n",
        "    self.targets = tf.placeholder(tf.int32, [None, self.num_classes], name=\"targets\")\n",
        "\n",
        "    # 노드를 보전할 확률\n",
        "    self.keep_prob = tf.placeholder(tf.float32, [], \"keep_prob\")\n",
        "\n",
        "\n",
        "  def _fnn_init(self):\n",
        "    with tf.name_scope(\"fnn_layer\"):\n",
        "      inputs = tf.reshape(self.inputs, shape=[-1, 28*28]) # 28*28 2차원 배열을 1*(28*28)일차원 배열로 만듬\n",
        "      \n",
        "      # (?, 28*28) -> (?, 10)\n",
        "      fully_connected_layer = tf.layers.dense(inputs=inputs, units=self.num_classes)\n",
        "    \n",
        "      self.outputs = fully_connected_layer    # (batch, num, classes)\n",
        "\n",
        "  def _predict_init(self):\n",
        "    # 각 클래스의 분포 값들을 softmax 함수를 이용하여 0~1사이의 값으로 변경\n",
        "    self.predict_op = tf.nn.softmax(logits=self.outputs, axis=-1) # softmax는 합쳐서 1이 되도록, 확률을 나타냄. ex) [6, 3, 1] -> [0.6, 0.3, 0.1]\n",
        "\n",
        "  def _train_init(self):\n",
        "    if self.mode == \"train\":\n",
        "      with tf.variable_scope(\"train_layer\"):\n",
        "        # 모델의 출력인 self.outputs 와 self.targets를 비교하여 loss 계산\n",
        "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.targets, logits=self.outputs)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.grads = self.optimizer.compute_gradients(self.loss)\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwrnE5li9bmF",
        "colab_type": "text"
      },
      "source": [
        "<h2>read_file, get_batch 함수</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path)</b>\n",
        "  \"mnist_train.csv\", \"mnist_test.csv\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    입력 이미지에 대한 픽셀값과 해당 이미지의 라벨을 담고 있는 리스트\n",
        "    \n",
        "  데이터 예시)\n",
        "    1, 0, 0, 0, 0, ... 0.6, 0.8, 0, 0, ... 0.7, 0.1, 0, 0, ... 0, 0, 0\n",
        "    라벨, 픽셀값, 픽셀값, ... 픽셀값\n",
        "</pre>  \n",
        "  \n",
        "  numpy array 함수 -> https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html <br>\n",
        "  numpy reshape 함수 -> https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html <br>\n",
        "  numpy zeros 함수 -> https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
        "  \n",
        "\n",
        "![대체 텍스트](http://nlp.kangwon.ac.kr/~nlpdemo/mnist_1.png)\n",
        "<pre>\n",
        "<b>2. get_batch(datas, batch_size)</b>\n",
        "  전체 데이터를 batch 단위로 나누어 주기 위한 함수\n",
        "  \n",
        "  get_batch(datas, batch_size)\n",
        "  args\n",
        "    datas : 입력 이미지에 대한 픽셀값과 해당 이미지의 라벨로 이루어진 리스트\n",
        "    batch_size : 한번에 학습할 데이터의 개수\n",
        "  return\n",
        "    batch 단위로 나뉘어진 데이터 리스트\n",
        "    \n",
        "  예시) \n",
        "    batch_size = 3 인 경우\n",
        "  \n",
        "    total_datas = [ (입력1, 라벨1), (입력2, 라벨2), (입력3, 라벨3), ... ,(입력10, 라벨10) ]\n",
        "    batches = [\n",
        "    [ (입력1, 입력2, 입력3), (라벨1, 라벨2, 라벨3)],\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hopD20S9VJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# 데이터를 읽고 픽셀값과 라벨을 분리하여 datas 리스트에 저장\n",
        "# 픽셀값을 (28, 28) 행렬로 변환\n",
        "# 라벨값은 one-hot encoding으로 변환\n",
        "# 예시) 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "def read_file(file_path):\n",
        "    inFile = open(file_path, \"r\", encoding=\"utf8\")\n",
        "    input_file = csv.reader(inFile)\n",
        "    lines = input_file\n",
        "\n",
        "    datas = []\n",
        "    ############################################################################\n",
        "    # 모범답안\n",
        "    for line in lines:\n",
        "      data = np.array(line[1:])\n",
        "      label = np.array(line[0])\n",
        "      index = int(label)\n",
        "      \n",
        "      data = np.reshape(data, (28, 28))\n",
        "      label = np.zeros(shape=(10))\n",
        "      label[index] = 1\n",
        "      \n",
        "      datas.append((data, label))\n",
        "\n",
        "#     내가 한것    \n",
        "#     x = np.zeros(10)\n",
        "#     x[lines[0]] = 1\n",
        "#     datas.append(x)\n",
        "#     datas.append(np.reshape(lines[1:], (28, 28)))    \n",
        "    ############################################################################\n",
        "\n",
        "    inFile.close()\n",
        "\n",
        "    return datas\n",
        "\n",
        "# 데이터를 batch 단위로 분할하여 저장\n",
        "def get_batch(datas, batch_size):\n",
        "  \n",
        "  # batches : batch 단위로 저장할 리스트\n",
        "  batches = []\n",
        "  ############################################################################\n",
        "  # 모범답안\n",
        "  x_datas, y_datas = [], []\n",
        "  for data, label in datas:\n",
        "    x_datas.append(data)\n",
        "    y_datas.append(label)\n",
        "    \n",
        "    if(len(x_datas) == batch_size):\n",
        "      batches.append((x_datas, y_datas))\n",
        "      x_datas, y_datas = [], []\n",
        "#   batches.append(np.reshape(datas, (-1, 3)))\n",
        "  \n",
        "\n",
        "  ############################################################################\n",
        "\n",
        "  return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnULUngdEoJb",
        "colab_type": "text"
      },
      "source": [
        "<h2>FNN 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>2. FNN 모델 객체 선언</b>\n",
        "\n",
        "<b>3. epoch를 돌때마다 학습 데이터 셔플</b>\n",
        "\n",
        "<b>3. batch 단위로 학습을 수행</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPLgRpO9SAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(flags):\n",
        "  # 학습 데이터 읽기\n",
        "  train_datas = read_file(flags[\"train_data_path\"])\n",
        "\n",
        "  # 모델 객체 선언\n",
        "  model = FNN(flags)\n",
        "  \n",
        "  # tensorflow session 옵션 설정\n",
        "  # allow_soft_placement=True : 어떤 device를 사용하여 연산할지 명시하지 않은 경우 자동으로 존재하는 디바이스 중에서 하나를 선택\n",
        "  # gpu_options=tf.GPUOptions(allow_growth=True) : 연산 실행 과정에서 필요한만큼의 gpu 메모리만 사용\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session \n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver(max_to_keep=10)          # 10개의 모델을 저장, 마지막이 최고가 아니라 중간에 나올수도 있으므로\n",
        "\n",
        "    for epoch in tqdm(range(flags[\"epoch\"])):\n",
        "      # 학습 데이터 셔플\n",
        "      np.random.shuffle(train_datas)\n",
        "      # 학습 데이터를 batch 단위로 분할하여 저장\n",
        "      batches = get_batch(train_datas, flags[\"batch_size\"])\n",
        "\n",
        "      losses = []\n",
        "      # batch 단위로 학습을 진행하며 각 batch 별 loss를 구한다\n",
        "      # batch 별 loss들의 평균을 구하여 이를 전체 데이터에 대한 loss로 사용\n",
        "      for train_x, train_y in batches:\n",
        "          loss, train_op = sess.run([model.loss, model.train_op],\n",
        "                                    feed_dict={ model.inputs:train_x, \n",
        "                                               model.targets:train_y, \n",
        "                                               model.keep_prob:flags[\"keep_prob\"] }\n",
        "                                    )\n",
        "          losses.append(loss)\n",
        "\n",
        "      # 학습한 모델 파일 저장\n",
        "      filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n",
        "      saver.save(sess, filename)\n",
        "\n",
        "      print(\"\\tEpoch : {}, Average_Loss : {}\".format(epoch+1, np.mean(losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeYeqCc_MUof",
        "colab_type": "text"
      },
      "source": [
        "<h2>FNN 모델 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>2. FNN 모델 객체 선언</b>\n",
        "\n",
        "<b>3. tf.train.Saver() 객체를 사용하여 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPLfTilw9MVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test(flags):\n",
        "  # 평가 데이터 읽기\n",
        "  test_datas = read_file(flags[\"test_data_path\"])\n",
        "\n",
        "  # 모델 객체 선언\n",
        "  model = FNN(flags)\n",
        "  # tensorflow session 옵션 설정\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session \n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n",
        "    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 분할하여 저장\n",
        "    batches = get_batch(test_datas, flags[\"batch_size\"])\n",
        "\n",
        "    # 정답을 맞춘 개수\n",
        "    correct_count = 0\n",
        "    for test_x, test_y in tqdm(batches):\n",
        "      predict_op = sess.run(model.predict_op,\n",
        "                            feed_dict={ model.inputs:test_x, model.keep_prob:flags[\"keep_prob\"] }\n",
        "                            )\n",
        "\n",
        "      # 모델의 outputs에는 각 클래스에 대한 분포가 저장되어 있고\n",
        "      # np.argmax 함수를 통하여 가장 확률이 높은 클래스를 선택\n",
        "      # 예시) \n",
        "      #  predict_op = [0,1, 0.3, 0.2] (각각 0일 확률, 1일 확률, 2일 확률)\n",
        "      #  np.argmax(predict_op) = 1\n",
        "      outputs, correct = np.argmax(predict_op, axis=-1), np.argmax(test_y, axis=-1)\n",
        "      correct_count += np.sum(np.equal(outputs, correct))\n",
        "\n",
        "    print(\"\\nAccuracy : \" + str(100.0*correct_count/(len(batches))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzkPsqYtNBHu",
        "colab_type": "text"
      },
      "source": [
        "<h2>모델의 hyper parameter 설정, 학습 및 평가 실행</h2>\n",
        "\n",
        "<pre>\n",
        "root_dir : 코드와 데이터가 있는 디렉토리 경로\n",
        "save_dir : 학습한 모델 파일을 저장할 디렉토리 경로(디렉토리가 존재하지 않을 경우 자동으로 생성)\n",
        "\n",
        "<b>flags : hyper parameter를 저장할 딕셔너리</b>\n",
        "  flags.mode = 학습 또는 평가 설정(\"train\" or \"test\")\n",
        "  flags.save_dir = 학습한 모델 파일을 저장할 디렉토리 경로\n",
        "  flags.batch_size = 한번에 학습할 데이터의 개수\n",
        "  flags.epoch = 학습 횟수\n",
        "  flags.learning_rate = 학습률\n",
        "  flags.keep_prob = 노드를 보전할 확률\n",
        "  flags.num_classes = 분류할 클래스의 개수(0~9, 따라서 10개)\n",
        "  flags.train_data_path = 학습데이터 파일 경로\n",
        "  flags.test_data_path = 평가데이터 파일 경로\n",
        "\n",
        "<b>mode 별 hyper parameter 변경</b>\n",
        "  학습하는 경우 : mode를 \"train\"으로 설정, 나머지는 기본 설정 그대로 유지\n",
        "  평가하는 경우 : mode를 \"test\"로, batch_size는 1로, keep_prob은 1.0으로 변경\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQfWrdn33utZ",
        "colab_type": "code",
        "outputId": "94902ff7-7dbe-4bb7-99b7-ba02a2c9defd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_dir = \"/gdrive/My Drive/colab/week 6\"\n",
        "  save_dir = os.path.join(root_dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"train\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":64,\n",
        "           \"epoch\":10,\n",
        "           \"learning_rate\":0.001,\n",
        "           \"keep_prob\":0.7,\n",
        "           \"num_classes\":10,\n",
        "           \"train_data_path\":os.path.join(root_dir, \"mnist_train.csv\"),\n",
        "           \"test_data_path\":os.path.join(root_dir, \"mnist_test.csv\")          \n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags)\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:07<01:05,  7.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 1, Average_Loss : 18.32221794128418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:13<00:56,  7.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 2, Average_Loss : 6.807912349700928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [00:20<00:48,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 3, Average_Loss : 5.286959171295166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [00:26<00:40,  6.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 4, Average_Loss : 4.599244117736816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [00:33<00:33,  6.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 5, Average_Loss : 4.217652320861816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [00:39<00:26,  6.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 6, Average_Loss : 4.079977989196777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [00:46<00:19,  6.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 7, Average_Loss : 3.687810182571411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [00:52<00:13,  6.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 8, Average_Loss : 3.600595235824585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [00:59<00:06,  6.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 9, Average_Loss : 3.439873695373535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [01:05<00:00,  6.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 10, Average_Loss : 3.246338367462158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPeU03-nvigz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "90a625cd-f2da-425d-a296-3318fc531dcc"
      },
      "source": [
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_dir = \"/gdrive/My Drive/colab/week 6\"\n",
        "  save_dir = os.path.join(root_dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"test\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":64,\n",
        "           \"epoch\":10,\n",
        "           \"learning_rate\":0.001,\n",
        "           \"keep_prob\":0.7,\n",
        "           \"num_classes\":10,\n",
        "           \"train_data_path\":os.path.join(root_dir, \"mnist_train.csv\"),\n",
        "           \"test_data_path\":os.path.join(root_dir, \"mnist_test.csv\")          \n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags)\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Read from : /gdrive/My Drive/colab/week 6/model/model_10.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:13<00:00, 750.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy : 86.72\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}