{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week8_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudht/NLP/blob/master/week8_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT0eZWL6TB7b",
        "colab_type": "code",
        "outputId": "0c847ae3-7a50-4a19-db4d-cf4726281aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53PregZ0iNHH",
        "colab_type": "text"
      },
      "source": [
        "<h2>Skip-gram을 이용한 Word2Vec</h2>\n",
        "\n",
        "![실습 그림](http://nlp.kangwon.ac.kr/~nlpdemo/skip_gram.png)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PpfOM6vq_V4",
        "colab_type": "code",
        "outputId": "2ac056a6-28ae-4e08-8770-50b06a177757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "dir = '/gdrive/My Drive/colab/week8'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh6CPgxqcZbk",
        "colab_type": "code",
        "outputId": "d59fa98d-566e-4c96-b4be-5f7eee2364a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# dictionary 생성\n",
        "idx2word = {0: \"One\", 1: \"Two\", 2: \"Three\", 3: \"Four\", 4: \"Five\",\n",
        "                       5: \"Six\", 6: \"Seven\", 7: \"Eight\", 8: \"Nine\"}\n",
        "word2idx = {\"One\":0,\"Two\":1,\"Three\":2,\"Four\":3,\"Five\":4,\"Six\":5,\"Seven\":6,\"Eight\":7,\"Nine\":8}\n",
        "\n",
        "# 난수로 데이터 생성하는 함수\n",
        "def make_data():\n",
        "  gen_datas = []\n",
        "  datas = []\n",
        "  for i in range(10000):\n",
        "    # ex)\n",
        "    #   odd_instance : \"One Five Nine\"\n",
        "    #   even_instance : \"Four Eight Two\"\n",
        "    odd_instance = \" \".join([idx2word[e-1] for e in np.random.choice(range(1, 10, 2), size=3)])\n",
        "    even_instance = \" \".join([idx2word[e-1] for e in np.random.choice(range(2, 10, 2), size=3)])\n",
        "    \n",
        "    gen_datas.append(odd_instance)\n",
        "    gen_datas.append(even_instance)\n",
        "\n",
        "  for instance_num in tqdm(range(len(gen_datas))):\n",
        "    line = gen_datas[instance_num].split()\n",
        "    # ex)\n",
        "    #   line : [\"One\", \"Five\", \"Nine\"]\n",
        "    #   datas : [(One, [Five]), (Five, [One]), (Five, [Nine]), ... ]\n",
        "    #   위 datas는 잘못된 예시임, (0, [5]) -> 이런 식으로 되어 있어야 함.\n",
        "    #   주변 단어에 대한 datas 생성\n",
        "\n",
        "    # 1. 위의 예시와 같이 line을 전처리 하여 datas에 추가하는 코드 작성\n",
        "    # datas는 list니깐 append 사용\n",
        "    ########################################################################\n",
        "    # 내 코드\n",
        "    # for idx, data in enumerate(line):\n",
        "    #   if(idx-1 >= 0):\n",
        "    #     datas.append((data,[line[idx-1]]))\n",
        "    #   if(idx+1 < len(line)):\n",
        "    #     datas.append((data,[line[idx+1]]))\n",
        "\n",
        "    # 정답 코드\n",
        "    for idx in range(len(line)):\n",
        "      if idx != 0:\n",
        "        datas.append((word2idx[line[idx]], [word2idx[line[idx-1]]]))\n",
        "      if idx != len(line)-1:\n",
        "        datas.append((word2idx[line[idx]], [word2idx[line[idx+1]]]))\n",
        "    ########################################################################\n",
        "  return datas\n",
        "\n",
        "# batch 단위로 나누어 데이터 반환\n",
        "def get_batch(datas, batch_size):\n",
        "  batches, inputs, outputs = [], [], []\n",
        "  \n",
        "  for input_idx, output_idx in datas:\n",
        "    inputs.append(input_idx)\n",
        "    outputs.append(output_idx)\n",
        "    \n",
        "    if len(inputs) == batch_size:\n",
        "      batches.append((inputs, outputs))\n",
        "      inputs = []\n",
        "      outputs = []\n",
        "  # batches : [([5,1,4,... ], [[4], [1], [6], ... ]), ([7, 6, 1, ... ], [[0], [1], [9], ... ]), ....]\n",
        "  return batches\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  datas = make_data()\n",
        "  print(np.shape(datas))\n",
        "  batches = get_batch(datas, 64)\n",
        "  print(np.shape(batches))\n",
        "  print(datas[0:10])\n",
        "  print(batches[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:00<00:00, 69537.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(80000, 2)\n",
            "(1250, 2, 64)\n",
            "[(2, [8]), (8, [2]), (8, [4]), (4, [8]), (7, [1]), (1, [7]), (1, [1]), (1, [1]), (2, [8]), (8, [2])]\n",
            "([2, 8, 8, 4, 7, 1, 1, 1, 2, 8, 8, 0, 1, 1, 1, 1, 6, 4, 4, 6, 1, 7, 7, 5, 8, 2, 2, 6, 7, 3, 3, 3, 8, 4, 4, 2, 5, 5, 5, 5, 2, 6, 6, 8, 5, 1, 1, 3, 4, 2, 2, 6, 1, 1, 1, 5, 8, 8, 8, 2, 3, 7, 7, 5], [[8], [2], [4], [8], [1], [7], [1], [1], [8], [2], [0], [8], [1], [1], [1], [1], [4], [6], [6], [4], [7], [1], [5], [7], [2], [8], [6], [2], [3], [7], [3], [3], [4], [8], [2], [4], [5], [5], [5], [5], [6], [2], [8], [6], [1], [5], [3], [1], [2], [4], [6], [2], [1], [1], [5], [1], [8], [8], [2], [8], [7], [3], [5], [7]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-W-jwUaS3h8",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](http://nlp.kangwon.ac.kr/~nlpdemo/word2vec.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aBxrH3e8kYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec():\n",
        "  def __init__(self, flags):\n",
        "    # flags : 하이퍼 파라미터\n",
        "\n",
        "    # 입력/출력 placeholder 선언\n",
        "    self.inputs = tf.placeholder(tf.int32, shape=[None], name='input_idx')\n",
        "    self.labels = tf.placeholder(tf.int32, shape=[None, 1], name='output_idx')\n",
        "    \n",
        "    # 하이퍼 파라미터 저장\n",
        "    self.embedding_size = flags['embedding_size']\n",
        "    self.vocab_size = flags['vocab_size']\n",
        "    self.learning_rate = flags['learning_rate']\n",
        "    self.num_samples = flags['num_samples']\n",
        "    \n",
        "    # build graph\n",
        "    self._embedding_init()\n",
        "    self._input_init()\n",
        "    self._train_init()\n",
        "  \n",
        "  def _embedding_init(self):\n",
        "    with tf.name_scope('embeddings'):\n",
        "      # projection matrix\n",
        "      self.embeddings = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0))\n",
        "      \n",
        "  def _input_init(self):\n",
        "    self.lookup_input = tf.nn.embedding_lookup(self.embeddings, self.inputs)\n",
        "  \n",
        "  def _train_init(self):\n",
        "    nce_weights = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0))\n",
        "    nce_biases = tf.Variable(tf.zeros([self.vocab_size]))\n",
        "\n",
        "    self.loss = tf.reduce_mean(\n",
        "        tf.nn.nce_loss(weights=nce_weights, biases=nce_biases,labels=self.labels, inputs=self.lookup_input,\n",
        "                       num_sampled=self.num_samples, num_classes=self.vocab_size))\n",
        "    self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz3Dd-pfEpAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def train(flags):\n",
        "  # 데이터 생성 및 저장\n",
        "  train_datas = make_data()\n",
        "\n",
        "  # 모델 선언\n",
        "  model = Word2Vec(flags)\n",
        "\n",
        "  # session 설절\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    for epoch in tqdm(range(flags['epoch'])):\n",
        "      # epoch 마다 data shuffle\n",
        "      random.shuffle(train_datas)\n",
        "      train_batches = get_batch(train_datas, flags['batch_size'])\n",
        "      for step in range(len(train_batches)):\n",
        "        train_input, train_label = train_batches[step]\n",
        "        \n",
        "        loss, _ = sess.run([model.loss, model.train_op], feed_dict = {\n",
        "            model.inputs : train_input,\n",
        "            model.labels : train_label\n",
        "            \n",
        "        })\n",
        "        \n",
        "        if (step+1) % flags['checkpoint_step'] == 0:\n",
        "          print(\"Epoch : {}\\tStep : {} / {}\\tCurrent Loss : {}\".format(epoch+1, step+1, len(train_batches), loss))\n",
        "      if (epoch+1) % flags['checkpoint_batch'] == 0:\n",
        "        filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n",
        "        saver.save(sess, filename)\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-snYKNaUQQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "def test(flags, word):\n",
        "  model = Word2Vec(flags)\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n",
        "    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n",
        "\n",
        "    # 학습된 word embedding을 가져옴\n",
        "    word_embedding = list(sess.run(model.embeddings))\n",
        "    input_word = word\n",
        "    input_idx = word2idx[input_word]\n",
        "    input_vector = word_embedding[input_idx]\n",
        "    \n",
        "    # 유사도와 vocab 단어 쌍을 저장하기 위한 리스트\n",
        "    # ex)\n",
        "    #   similarity_list : [(0, 0.5), (1, 0.8), ... ]\n",
        "    similarity_list = []\n",
        "    \n",
        "    # 1. 입력 단어와 단어장에 있는 단어들과의 유사도 구하기 (Cosine 유사도 수식, numpy 활용)\n",
        "    #   ex)\n",
        "    #     input_word : One\n",
        "    #     input_idx : 0\n",
        "    #     input_vector : [0.654, 0.414, -0.654, ... ]\n",
        "    #     np.dot(A, B)\n",
        "    ###########################################################\n",
        "    for idx, e in enumerate(word_embedding):\n",
        "      if idx == input_idx:\n",
        "        continue\n",
        "      sim = np.dot(e, input_vector)\n",
        "      similarity_list.append((idx, sim))\n",
        "    ###########################################################\n",
        "\n",
        "    # 유사도 순으로 내림차순 정렬\n",
        "    similarity_list = sorted(similarity_list, key=operator.itemgetter(1), reverse=True)\n",
        "    \n",
        "    # 유사 단어 출력\n",
        "    for w in similarity_list[:3]:\n",
        "      print(idx2word[w[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo10qHTFHM_6",
        "colab_type": "code",
        "outputId": "806ec451-fc90-45f1-de90-2b777ebccc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  save_dir = os.path.join(dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"train\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":64,\n",
        "           \"epoch\":10,\n",
        "           \"learning_rate\":0.01,\n",
        "           \"embedding_size\":50,\n",
        "           \"vocab_size\":9,\n",
        "           \"num_samples\" : 8,\n",
        "           \"checkpoint_step\" : 100,\n",
        "           \"checkpoint_batch\" : 5\n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags, 'Five')\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:00<00:00, 96209.23it/s]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1\tStep : 100 / 1250\tCurrent Loss : 2.576582193374634\n",
            "Epoch : 1\tStep : 200 / 1250\tCurrent Loss : 2.5796260833740234\n",
            "Epoch : 1\tStep : 300 / 1250\tCurrent Loss : 2.60315203666687\n",
            "Epoch : 1\tStep : 400 / 1250\tCurrent Loss : 2.6017422676086426\n",
            "Epoch : 1\tStep : 500 / 1250\tCurrent Loss : 2.5668158531188965\n",
            "Epoch : 1\tStep : 600 / 1250\tCurrent Loss : 2.6019721031188965\n",
            "Epoch : 1\tStep : 700 / 1250\tCurrent Loss : 2.6208906173706055\n",
            "Epoch : 1\tStep : 800 / 1250\tCurrent Loss : 2.57877516746521\n",
            "Epoch : 1\tStep : 900 / 1250\tCurrent Loss : 2.606841802597046\n",
            "Epoch : 1\tStep : 1000 / 1250\tCurrent Loss : 2.5264673233032227\n",
            "Epoch : 1\tStep : 1100 / 1250\tCurrent Loss : 2.511291265487671\n",
            "Epoch : 1\tStep : 1200 / 1250\tCurrent Loss : 2.5020642280578613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:11,  1.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 2\tStep : 100 / 1250\tCurrent Loss : 2.4854350090026855\n",
            "Epoch : 2\tStep : 200 / 1250\tCurrent Loss : 2.4490857124328613\n",
            "Epoch : 2\tStep : 300 / 1250\tCurrent Loss : 2.4568326473236084\n",
            "Epoch : 2\tStep : 400 / 1250\tCurrent Loss : 2.572140693664551\n",
            "Epoch : 2\tStep : 500 / 1250\tCurrent Loss : 2.504519462585449\n",
            "Epoch : 2\tStep : 600 / 1250\tCurrent Loss : 2.5103964805603027\n",
            "Epoch : 2\tStep : 700 / 1250\tCurrent Loss : 2.62953519821167\n",
            "Epoch : 2\tStep : 800 / 1250\tCurrent Loss : 2.495878219604492\n",
            "Epoch : 2\tStep : 900 / 1250\tCurrent Loss : 2.479996919631958\n",
            "Epoch : 2\tStep : 1000 / 1250\tCurrent Loss : 2.4314682483673096\n",
            "Epoch : 2\tStep : 1100 / 1250\tCurrent Loss : 2.544127941131592\n",
            "Epoch : 2\tStep : 1200 / 1250\tCurrent Loss : 2.5064282417297363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:02<00:10,  1.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 3\tStep : 100 / 1250\tCurrent Loss : 2.5119829177856445\n",
            "Epoch : 3\tStep : 200 / 1250\tCurrent Loss : 2.560089111328125\n",
            "Epoch : 3\tStep : 300 / 1250\tCurrent Loss : 2.486729621887207\n",
            "Epoch : 3\tStep : 400 / 1250\tCurrent Loss : 2.5939793586730957\n",
            "Epoch : 3\tStep : 500 / 1250\tCurrent Loss : 2.5137686729431152\n",
            "Epoch : 3\tStep : 600 / 1250\tCurrent Loss : 2.5074546337127686\n",
            "Epoch : 3\tStep : 700 / 1250\tCurrent Loss : 2.4948513507843018\n",
            "Epoch : 3\tStep : 800 / 1250\tCurrent Loss : 2.616584300994873\n",
            "Epoch : 3\tStep : 900 / 1250\tCurrent Loss : 2.527844190597534\n",
            "Epoch : 3\tStep : 1000 / 1250\tCurrent Loss : 2.562437057495117\n",
            "Epoch : 3\tStep : 1100 / 1250\tCurrent Loss : 2.4972145557403564\n",
            "Epoch : 3\tStep : 1200 / 1250\tCurrent Loss : 2.578514575958252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [00:03<00:08,  1.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 4\tStep : 100 / 1250\tCurrent Loss : 2.439741611480713\n",
            "Epoch : 4\tStep : 200 / 1250\tCurrent Loss : 2.6071994304656982\n",
            "Epoch : 4\tStep : 300 / 1250\tCurrent Loss : 2.514604091644287\n",
            "Epoch : 4\tStep : 400 / 1250\tCurrent Loss : 2.512619733810425\n",
            "Epoch : 4\tStep : 500 / 1250\tCurrent Loss : 2.5071873664855957\n",
            "Epoch : 4\tStep : 600 / 1250\tCurrent Loss : 2.526927947998047\n",
            "Epoch : 4\tStep : 700 / 1250\tCurrent Loss : 2.58247971534729\n",
            "Epoch : 4\tStep : 800 / 1250\tCurrent Loss : 2.48861026763916\n",
            "Epoch : 4\tStep : 900 / 1250\tCurrent Loss : 2.5583577156066895\n",
            "Epoch : 4\tStep : 1000 / 1250\tCurrent Loss : 2.4899370670318604\n",
            "Epoch : 4\tStep : 1100 / 1250\tCurrent Loss : 2.5404529571533203\n",
            "Epoch : 4\tStep : 1200 / 1250\tCurrent Loss : 2.5748353004455566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [00:05<00:07,  1.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 5\tStep : 100 / 1250\tCurrent Loss : 2.5413126945495605\n",
            "Epoch : 5\tStep : 200 / 1250\tCurrent Loss : 2.5613417625427246\n",
            "Epoch : 5\tStep : 300 / 1250\tCurrent Loss : 2.5253634452819824\n",
            "Epoch : 5\tStep : 400 / 1250\tCurrent Loss : 2.5540192127227783\n",
            "Epoch : 5\tStep : 500 / 1250\tCurrent Loss : 2.520552158355713\n",
            "Epoch : 5\tStep : 600 / 1250\tCurrent Loss : 2.540661573410034\n",
            "Epoch : 5\tStep : 700 / 1250\tCurrent Loss : 2.4728081226348877\n",
            "Epoch : 5\tStep : 800 / 1250\tCurrent Loss : 2.491978645324707\n",
            "Epoch : 5\tStep : 900 / 1250\tCurrent Loss : 2.502162456512451\n",
            "Epoch : 5\tStep : 1000 / 1250\tCurrent Loss : 2.4418091773986816\n",
            "Epoch : 5\tStep : 1100 / 1250\tCurrent Loss : 2.495913028717041\n",
            "Epoch : 5\tStep : 1200 / 1250\tCurrent Loss : 2.513103485107422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [00:06<00:06,  1.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 6\tStep : 100 / 1250\tCurrent Loss : 2.489645004272461\n",
            "Epoch : 6\tStep : 200 / 1250\tCurrent Loss : 2.5282704830169678\n",
            "Epoch : 6\tStep : 300 / 1250\tCurrent Loss : 2.504115104675293\n",
            "Epoch : 6\tStep : 400 / 1250\tCurrent Loss : 2.5245299339294434\n",
            "Epoch : 6\tStep : 500 / 1250\tCurrent Loss : 2.4327616691589355\n",
            "Epoch : 6\tStep : 600 / 1250\tCurrent Loss : 2.481529712677002\n",
            "Epoch : 6\tStep : 700 / 1250\tCurrent Loss : 2.603593587875366\n",
            "Epoch : 6\tStep : 800 / 1250\tCurrent Loss : 2.5373055934906006\n",
            "Epoch : 6\tStep : 900 / 1250\tCurrent Loss : 2.4136013984680176\n",
            "Epoch : 6\tStep : 1000 / 1250\tCurrent Loss : 2.4926064014434814\n",
            "Epoch : 6\tStep : 1100 / 1250\tCurrent Loss : 2.5948421955108643\n",
            "Epoch : 6\tStep : 1200 / 1250\tCurrent Loss : 2.550572395324707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [00:07<00:05,  1.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 7\tStep : 100 / 1250\tCurrent Loss : 2.609574556350708\n",
            "Epoch : 7\tStep : 200 / 1250\tCurrent Loss : 2.530120849609375\n",
            "Epoch : 7\tStep : 300 / 1250\tCurrent Loss : 2.5141329765319824\n",
            "Epoch : 7\tStep : 400 / 1250\tCurrent Loss : 2.5133414268493652\n",
            "Epoch : 7\tStep : 500 / 1250\tCurrent Loss : 2.5334134101867676\n",
            "Epoch : 7\tStep : 600 / 1250\tCurrent Loss : 2.5364904403686523\n",
            "Epoch : 7\tStep : 700 / 1250\tCurrent Loss : 2.480271100997925\n",
            "Epoch : 7\tStep : 800 / 1250\tCurrent Loss : 2.5663108825683594\n",
            "Epoch : 7\tStep : 900 / 1250\tCurrent Loss : 2.531285285949707\n",
            "Epoch : 7\tStep : 1000 / 1250\tCurrent Loss : 2.508761405944824\n",
            "Epoch : 7\tStep : 1100 / 1250\tCurrent Loss : 2.504831552505493\n",
            "Epoch : 7\tStep : 1200 / 1250\tCurrent Loss : 2.552656650543213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [00:08<00:03,  1.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 8\tStep : 100 / 1250\tCurrent Loss : 2.547469139099121\n",
            "Epoch : 8\tStep : 200 / 1250\tCurrent Loss : 2.531250476837158\n",
            "Epoch : 8\tStep : 300 / 1250\tCurrent Loss : 2.5002291202545166\n",
            "Epoch : 8\tStep : 400 / 1250\tCurrent Loss : 2.5091867446899414\n",
            "Epoch : 8\tStep : 500 / 1250\tCurrent Loss : 2.5915489196777344\n",
            "Epoch : 8\tStep : 600 / 1250\tCurrent Loss : 2.5402321815490723\n",
            "Epoch : 8\tStep : 700 / 1250\tCurrent Loss : 2.539841890335083\n",
            "Epoch : 8\tStep : 800 / 1250\tCurrent Loss : 2.5321407318115234\n",
            "Epoch : 8\tStep : 900 / 1250\tCurrent Loss : 2.525205135345459\n",
            "Epoch : 8\tStep : 1000 / 1250\tCurrent Loss : 2.528102397918701\n",
            "Epoch : 8\tStep : 1100 / 1250\tCurrent Loss : 2.5278332233428955\n",
            "Epoch : 8\tStep : 1200 / 1250\tCurrent Loss : 2.506202459335327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [00:10<00:02,  1.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 9\tStep : 100 / 1250\tCurrent Loss : 2.5127315521240234\n",
            "Epoch : 9\tStep : 200 / 1250\tCurrent Loss : 2.507315158843994\n",
            "Epoch : 9\tStep : 300 / 1250\tCurrent Loss : 2.5484097003936768\n",
            "Epoch : 9\tStep : 400 / 1250\tCurrent Loss : 2.511488914489746\n",
            "Epoch : 9\tStep : 500 / 1250\tCurrent Loss : 2.5173988342285156\n",
            "Epoch : 9\tStep : 600 / 1250\tCurrent Loss : 2.5136427879333496\n",
            "Epoch : 9\tStep : 700 / 1250\tCurrent Loss : 2.581486940383911\n",
            "Epoch : 9\tStep : 800 / 1250\tCurrent Loss : 2.5482120513916016\n",
            "Epoch : 9\tStep : 900 / 1250\tCurrent Loss : 2.521885395050049\n",
            "Epoch : 9\tStep : 1000 / 1250\tCurrent Loss : 2.5422661304473877\n",
            "Epoch : 9\tStep : 1100 / 1250\tCurrent Loss : 2.505429744720459\n",
            "Epoch : 9\tStep : 1200 / 1250\tCurrent Loss : 2.4855856895446777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [00:11<00:01,  1.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 10\tStep : 100 / 1250\tCurrent Loss : 2.530263900756836\n",
            "Epoch : 10\tStep : 200 / 1250\tCurrent Loss : 2.536910057067871\n",
            "Epoch : 10\tStep : 300 / 1250\tCurrent Loss : 2.536465883255005\n",
            "Epoch : 10\tStep : 400 / 1250\tCurrent Loss : 2.4859542846679688\n",
            "Epoch : 10\tStep : 500 / 1250\tCurrent Loss : 2.4987032413482666\n",
            "Epoch : 10\tStep : 600 / 1250\tCurrent Loss : 2.5173709392547607\n",
            "Epoch : 10\tStep : 700 / 1250\tCurrent Loss : 2.4975779056549072\n",
            "Epoch : 10\tStep : 800 / 1250\tCurrent Loss : 2.555988311767578\n",
            "Epoch : 10\tStep : 900 / 1250\tCurrent Loss : 2.5574698448181152\n",
            "Epoch : 10\tStep : 1000 / 1250\tCurrent Loss : 2.5444741249084473\n",
            "Epoch : 10\tStep : 1100 / 1250\tCurrent Loss : 2.5752358436584473\n",
            "Epoch : 10\tStep : 1200 / 1250\tCurrent Loss : 2.495375633239746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [00:12<00:00,  1.27s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}