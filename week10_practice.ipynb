{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week10_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudht/NLP/blob/master/week10_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9",
        "colab_type": "text"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-lNQfT1D1v",
        "colab_type": "code",
        "outputId": "e232b454-5b85-425b-9cfe-a300e6425305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWZLECjoJkn",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델</h2>\n",
        "\n",
        "![실습 그림](http://nlp.kangwon.ac.kr/~nlpdemo/ner_figure.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5861idd25QB",
        "colab_type": "code",
        "outputId": "af73285c-4bc9-4f81-aabb-e73d9d2a7491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "class RNN(object):\n",
        "  \n",
        "  def __init__(self, flags):\n",
        "    self.encoder_vocab_size = flags[\"encoder_vocab_size\"]    # 음절 개수\n",
        "    self.label_vocab_size = flags[\"label_vocab_size\"]\n",
        "    self.embedding_size = flags[\"embedding_size\"]            # 음절 임베딩 사이즈\n",
        "    self.hidden_size = flags[\"hidden_size\"]                  # GRU 히든 사이즈\n",
        "\n",
        "    self.max_length = flags[\"max_length\"]                    # 음절 최대 길이\n",
        "    self.keep_prob = flags[\"keep_prob\"]                      # 노드를 보전할 확률\n",
        "    self.learning_rate = flags[\"learning_rate\"]              # 학습률\n",
        "    self.mode = flags[\"mode\"]                                # 학습 or 평가 상태\n",
        "\n",
        "    self._input_init()\n",
        "    self._embedding_init()\n",
        "    self._rnn_init()\n",
        "    self._train_init()\n",
        "    self._predict_init()\n",
        "        \n",
        "  # 입력 데이터, 입력 데이터의 길이, 정답 데이터, keep_prob 값을 담을 tensor 선언  \n",
        "  def _input_init(self):\n",
        "    # 입력 데이터\n",
        "    self.inputs = tf.placeholder(tf.int32, [None, self.max_length], name=\"inputs\")\n",
        "    \n",
        "    # 입력 데이터의 길이\n",
        "    self.inputs_length = tf.placeholder(tf.int32, [None], name=\"inputs_length\")\n",
        "\n",
        "    # 정답 데이터\n",
        "    self.targets = tf.placeholder(tf.int32, [None, self.max_length], name=\"targets\")\n",
        "    # loss를 구할 때 각 sequence의 가중치를 설정\n",
        "    self.target_weights = tf.sequence_mask(lengths=self.inputs_length, maxlen=self.max_length, dtype=tf.float32, name='weight')\n",
        "    # 노드를 보전할 확률\n",
        "    self.keep_prob = tf.placeholder(tf.float32, [], \"keep_prob\")\n",
        "\n",
        "  def _embedding_init(self):\n",
        "    # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 tensor\n",
        "    # 기존에 사전학습 된 음절 임베딩을 사용할 수도 있고 랜덤으로 초기화 한 후,\n",
        "    # 모델 학습 과정 중에 같이 학습 시키는 것도 가능\n",
        "    # 예제 코드는 랜덤으로 초기화 한 후 같이 학습하도록 설정\n",
        "    self.encoder_embeddings = tf.get_variable(\"encoder_embedding\",\n",
        "                                             shape = [self.encoder_vocab_size, self.embedding_size],\n",
        "                                             dtype = tf.float32, trainable=True,\n",
        "                                             initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "\n",
        "  def _rnn_init(self):\n",
        "    with tf.name_scope(\"rnn_layer\"):\n",
        "      # rnn에서 사용할 cell 설정\n",
        "      # 예제 코드에서는 bidirectional lstm를 사용하기 때문에 정방향 cell과 역방향 cell 두개를 사용\n",
        "      fw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      bw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      \n",
        "      # 임베딩 tensor를 이용하여 입력데이터의 각 음절 index를 대응하는 임베딩 벡터로 치환\n",
        "      # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "      self.lookup_inputs = tf.nn.embedding_lookup(self.encoder_embeddings, self.inputs)\n",
        "\n",
        "      # tf.nn.bidirectional_dynamic_rnn 라이브러리를 사용하여 rnn layer의 출력을 구한다\n",
        "      # outputs : 각 step의 rnn 출력값, state : 마지막 hidden state 값\n",
        "      (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, self.lookup_inputs,\n",
        "                                                                                       sequence_length=self.inputs_length,\n",
        "                                                                                       dtype=tf.float32,\n",
        "                                                                                       time_major=False)\n",
        "\n",
        "      # 예제 코드에서는 각 step의 출력값을 사용한다.\n",
        "      # 양방향으로 생성된 rnn 결과를 연결하여 하나의 벡터로 표현\n",
        "      # (batch_size, max_length, hidden_size)*2 -> (batch_size, max_length, hidden_size*2)\n",
        "      outputs = tf.concat([fw_outputs, bw_outputs], axis=2)\n",
        "      \n",
        "      # fully_connected layer를 통하여 출력 크기를 label_vocab_size에 맞춰줌\n",
        "      # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, label_vocab_size)\n",
        "      self.outputs = tf.layers.dense(inputs=outputs,\n",
        "                                     units=self.label_vocab_size,\n",
        "                                     activation=tf.nn.relu,\n",
        "                                     use_bias=False,\n",
        "                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "    \n",
        "    \n",
        "  def _predict_init(self):\n",
        "    # 각 라벨의 분포 값들을 softmax 함수를 이용하여 0~1사이의 값으로 변경\n",
        "    self.predict_op = tf.nn.softmax(logits=self.outputs, axis=-1)\n",
        "\n",
        "  def _train_init(self):\n",
        "    if self.mode == \"train\":\n",
        "      with tf.variable_scope(\"train_layer\"):\n",
        "        # 모델의 출력인 self.outputs 와 self.targets를 비교하여 loss 계산\n",
        "        self.loss = tf.contrib.seq2seq.sequence_loss(self.outputs, self.targets, self.target_weights)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.grads = self.optimizer.compute_gradients(self.loss)\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwrnE5li9bmF",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 읽고 전처리 하기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path)</b>\n",
        "  \"train_datas.txt\", \"test_datas.txt\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  데이터 예시)\n",
        "    대 한 민 국 은 〈SP〉민 주 주 의 국 가 이 다 . \\t B_OG I I I O 〈SP〉O O O O O O O O O\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    \n",
        "    출력 예시)\n",
        "      [\"대\", \"한\", \"민\", \"국\", \"은\", \"〈SP〉\",  \"민\", \"주\", \"주\", \"의\", \"국\", \"가\", \"이\", \"다\", \".\"], 15, [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "      \n",
        "<b>2. read_vocab_file(flags)</b>\n",
        "  \"eumjeol_vocab.txt\", \"label_vocab.txt\" 파일을 읽고 음절과 라벨을 indexing하기 위한 딕셔너리를 생성\n",
        "   \n",
        "  read_vocab_file(flags)\n",
        "  args\n",
        "    flags : hyperparameter들을 담고 있는 딕셔너리\n",
        "  return\n",
        "    eumjeol2idx, idx2eumjeol, label2idx, idx2label 딕셔너리\n",
        "    \n",
        "    eumjeol2idx : 음절을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환하기 위한 딕셔너리\n",
        "    label2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환하기 위한 딕셔너리\n",
        "    \n",
        "<b>3. prepro(datas, eumjeol2idx, label2idx)</b>\n",
        "  입력 데이터의 각 음절과 라벨을 indexing, 음절 sequence의 길이를 고정된 길이로 변환\n",
        "   \n",
        "  prepro(datas, eumjeol2idx, label2idx)\n",
        "  args\n",
        "    file_path : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "  return\n",
        "    입력 데이터의 각 음절을 indexing하고 라벨을 one_hot 인코딩한 리스트\n",
        "    \n",
        "  전처리 예시)\n",
        "    [\"대\", \"한\", \"민\", \"국\", \"은\", \"〈SP〉\",  \"민\", \"주\", \"주\", \"의\", \"국\", \"가\", \"이\", \"다\", \".\"], 15, [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    \n",
        "    사전 설정한 문장의 최대 길이를 20이라고 가정\n",
        "    \n",
        "     [\"대\", \"한\", \"민\", \"국\", \"은\", \"〈SP〉\",  \"민\", \"주\", \"주\", \"의\", \"국\", \"가\", \"이\", \"다\", \".\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\"]\n",
        "         [\"대\", \"한\", \"민\", \"국\", \"은\", \"〈SP〉\",  \"민\", \"주\", \"주\", \"의\", \"국\", \"가\", \"이\", \"다\", \".\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\"] -> [ 23, 2, 55, 65, 96, 12, 4, 11, 235, 5, 3, 12, 98, 1, 0, 0, 0, 0, 0, 0]\n",
        "    \n",
        "    [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"] -> [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\"]\n",
        "    [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\", \"〈PAD〉\"] -> [ 7, 2, 2, 2, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0 ]    \n",
        " </pre>\n",
        "        \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hopD20S9VJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 데이터를 읽고 음절 sequence와 라벨 sequence를 분리\n",
        "# 음절 sequence의 길이를 구하고 음절 sequence, 음절 sequence의 길이, 라벨 sequence를 datas 리스트에 저장\n",
        "def read_file(file_path):\n",
        "  with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "    lines = inFile.readlines()\n",
        "  datas = []\n",
        "  for line in lines:\n",
        "    # 입력 문장을 \\t으로 분리\n",
        "    pieces = line.strip().split(\"\\t\")\n",
        "    eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n",
        "    eumjeol_length = len(eumjeol_sequence)\n",
        "\n",
        "    datas.append((eumjeol_sequence, eumjeol_length, label_sequence))\n",
        "\n",
        "  return datas\n",
        "  \n",
        "def read_vocab_file(flags):\n",
        "  eumjeol2idx, idx2eumjeol = {\"<PAD>\":0}, {0:\"<PAD>\"}\n",
        "  label2idx, idx2label = {\"<PAD>\":0}, {0:\"<PAD>\"}\n",
        "  \n",
        "  with open(flags[\"eumjeol_vocab_data_path\"], \"r\", encoding=\"utf8\") as inFile:\n",
        "    eumjeols = inFile.readlines()\n",
        "  with open(flags[\"label_vocab_data_path\"], \"r\", encoding=\"utf8\") as inFile:\n",
        "    labels = inFile.readlines()\n",
        "\n",
        "  for eumjeol in eumjeols:\n",
        "    eumjeol = eumjeol.strip()\n",
        "\n",
        "    if(eumjeol not in eumjeol2idx):\n",
        "      eumjeol2idx[eumjeol] = len(eumjeol2idx)\n",
        "\n",
        "    if(eumjeol2idx[eumjeol] not in idx2eumjeol):\n",
        "      idx2eumjeol[eumjeol2idx[eumjeol]] = eumjeol\n",
        "\n",
        "  for label in labels:\n",
        "    label = label.strip()\n",
        "\n",
        "    if(label not in label2idx):\n",
        "      label2idx[label] = len(label2idx)\n",
        "\n",
        "    if(label2idx[label] not in idx2label):\n",
        "      idx2label[label2idx[label]] = label\n",
        "      \n",
        "  return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n",
        "        \n",
        "# 입력 데이터 전처리 \n",
        "def prepro(datas, eumjeol2idx, label2idx, max_length):\n",
        "  \n",
        "  preprocessed_datas = []\n",
        "  for eumjeol_sequence, eumjeol_length, label_sequence in datas:\n",
        "    \n",
        "    # max_length의 길이를 가지는 numpy array 생성하고 0으로 초기화\n",
        "    indexing_eumjeol_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    indexing_label_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    # 각 음절과 라벨 index를 numpy array에 입력\n",
        "    for index in range(eumjeol_length):\n",
        "      indexing_eumjeol_sequence[index] = eumjeol2idx[eumjeol_sequence[index]]\n",
        "      indexing_label_sequence[index] = label2idx[label_sequence[index]]\n",
        "    preprocessed_datas.append((indexing_eumjeol_sequence, eumjeol_length, indexing_label_sequence))\n",
        "  return preprocessed_datas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTgxrex_qW--",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 batch 단위로 나누기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>get_batch(datas, batch_size)</b>\n",
        "  전체 데이터를 batch 단위로 나누어 주기 위한 함수\n",
        "  \n",
        "  get_batch(datas, batch_size)\n",
        "  args\n",
        "    datas : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    batch_size : 한번에 학습할 데이터의 개수\n",
        "  return\n",
        "    batch 단위로 나뉘어진 데이터 리스트\n",
        "    \n",
        "  예시) \n",
        "    batch_size = 3 인 경우\n",
        "  \n",
        "    total_datas = [ (음절 sequence1, sequence1의 길이, 라벨 sequence1), ... , ((음절 sequence100, sequence100의 길이, 라벨 sequence100)) ]\n",
        "    \n",
        "    batches = [\n",
        "    [ [ 음절 sequence1, 음절 sequence2, 음절 sequence3 ], \n",
        "      [ sequence1의 길이, sequence2의 길이, sequence3의 길이 ],\n",
        "      [ 라벨 sequence1, 라벨 sequence2, 라벨 sequence3 ]\n",
        "    ],\n",
        "    \n",
        "    [ [ 음절 sequence4, 음절 sequence5, 음절 sequence6 ], \n",
        "      [ sequence4의 길이, sequence5의 길이, sequence6의 길이 ],\n",
        "      [ 라벨 sequence4, 라벨 sequence5, 라벨 sequence6 ]\n",
        "    ],\n",
        "    \n",
        "    ...\n",
        "    \n",
        "    ]\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG3GWKrAqQzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터를 batch 단위로 분할하여 저장\n",
        "def get_batch(datas, batch_size):\n",
        "  # batches : batch 단위로 저장할 리스트\n",
        "  # inputs : 각 batch 단위 별 음절 sequence를 저장할 리스트\n",
        "  # inputs_length : 각 batch 단위 별 음절 sequence 길이를 저장할 리스트\n",
        "  # targets : 각 batch 단위 별 라벨 sequence를 저장할 리스트\n",
        "  batches, inputs, inputs_length, targets = [], [], [], []\n",
        "    \n",
        "  for indexing_eumjeol_sequence, eumjeol_length, indexing_label_sequence in datas:\n",
        "    inputs.append(indexing_eumjeol_sequence)\n",
        "    inputs_length.append(eumjeol_length)\n",
        "    targets.append(indexing_label_sequence)\n",
        "    \n",
        "    if(len(inputs) == batch_size):\n",
        "      batches.append((inputs, inputs_length, targets))\n",
        "      inputs, inputs_length, targets = [], [], []\n",
        "\n",
        "  return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4uOGj7vwFFY",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(flags) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 학습 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>3. epoch를 돌때마다 학습 데이터 셔플</b>\n",
        "\n",
        "<b>3. batch 단위로 학습을 수행</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPLgRpO9SAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags)\n",
        "  # 학습 데이터 읽기\n",
        "  train_datas = read_file(flags[\"train_data_path\"])\n",
        "  # 학습 데이터 전처리\n",
        "  preprocessed_train_datas = prepro(train_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  \n",
        "  # tensorflow session 옵션 설정\n",
        "  # allow_soft_placement=True : 어떤 device를 사용하여 연산할지 명시하지 않은 경우 자동으로 존재하는 디바이스 중에서 하나를 선택\n",
        "  # gpu_options=tf.GPUOptions(allow_growth=True) : 연산 실행 과정에서 필요한만큼의 gpu 메모리만 사용\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    for epoch in tqdm(range(flags[\"epoch\"])):\n",
        "      # 학습 데이터 셔플\n",
        "      np.random.shuffle(preprocessed_train_datas)\n",
        "      # 학습 데이터를 batch 단위로 분할하여 저장\n",
        "      batches = get_batch(preprocessed_train_datas, flags[\"batch_size\"])\n",
        "\n",
        "      losses = []\n",
        "      # batch 단위로 학습을 진행하며 각 batch 별 loss를 구한다\n",
        "      # batch 별 loss들의 평균을 구하여 이를 전체 데이터에 대한 loss로 사용\n",
        "      for inputs, inputs_length, targets in batches:\n",
        "        loss, train_op = sess.run([model.loss, model.train_op],\n",
        "                                  feed_dict={ model.inputs:inputs, \n",
        "                                             model.inputs_length:inputs_length, \n",
        "                                             model.targets:targets, \n",
        "                                             model.keep_prob:flags[\"keep_prob\"] }\n",
        "                                 )\n",
        "\n",
        "        losses.append(loss)\n",
        "        \n",
        "      # 학습한 모델 파일 저장\n",
        "      filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n",
        "      saver.save(sess, filename)\n",
        "\n",
        "      print(\"\\tEpoch : {}, Average_Loss : {}\".format(epoch+1, np.mean(losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wzroGrw2xm",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(flags) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 평가 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>5. tf.train.Saver() 객체를 사용하여 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴</b>\n",
        "\n",
        "<b>6. make_sentence(inputs, predict, correct, idx2eumjeol, idx2label) 함수를 이용하여 정답과 모델 출력 비교</b>\n",
        "  \n",
        "  make_sentence(inputs, predict, correct, idx2eumjeol, idx2label)\n",
        "  args\n",
        "    inputs : 음절 sequence\n",
        "    predict : 모델 출력 라벨 sequence\n",
        "    correct : 정답 라벨 sequence\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환해주는 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환해주는 딕셔너리\n",
        "  return\n",
        "    정답 문장과 모델 출력 문장\n",
        "\n",
        "  예시)\n",
        "    inputs = [\"대\", \"한\", \"민\", \"국\", \"은\", \"〈SP〉\",  \"민\", \"주\", \"주\", \"의\", \"국\", \"가\", \"이\", \"다\", \".\"]\n",
        "    predict = [\"B_LC\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    correct = [\"B_OG\", \"I\", \"I\", \"I\", \"O\", \"〈SP〉\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    \n",
        "    정답 문장 : <대한민국:OG>은 민주주의국가이다.\n",
        "    모델 출력 문장 : <대한민국:LC>은 민주주의 국가이다.\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPLfTilw9MVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 모델 출력 라벨 sequence와 정답 라벨 sequence를 기반으로\n",
        "# 모델 출력 문장과 정답 문장 출력\n",
        "def make_sentence(inputs, label_idx_list, idx2eumjeol, idx2label):\n",
        "  \n",
        "  # 빈 문자열 생성\n",
        "  flag = False\n",
        "  restored_sentence = \"\"\n",
        "  ner = \"\"\n",
        "  tag = \"\"\n",
        "  for index in range(len(label_idx_list)):\n",
        "    eumjeol = idx2eumjeol[inputs[index]]\n",
        "    label = idx2label[label_idx_list[index]]\n",
        "    if label[0] == 'B':\n",
        "      #####################################################\n",
        "      flag = True\n",
        "      restored_sentence += (\"<\" + eumjeol)\n",
        "      tag += \":\" + label[2] + label[3] + \">\"\n",
        "      #####################################################\n",
        "\n",
        "    elif label == 'I':\n",
        "      #####################################################\n",
        "      restored_sentence += eumjeol\n",
        "      #####################################################\n",
        "\n",
        "    elif label == 'O':\n",
        "      #####################################################\n",
        "      if flag == True:\n",
        "        restored_sentence += tag\n",
        "        flag = False\n",
        "        tag = \"\"\n",
        "      restored_sentence += eumjeol\n",
        "      #####################################################\n",
        "\n",
        "    else:\n",
        "      #####################################################\n",
        "      if flag == True:\n",
        "        restored_sentence += tag\n",
        "        flag = False\n",
        "        tag = \"\"\n",
        "      restored_sentence += \" \"\n",
        "      #####################################################\n",
        "  return restored_sentence\n",
        "      \n",
        "\n",
        "def test(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags)\n",
        "  # 평가 데이터 읽기\n",
        "  test_datas = read_file(flags[\"test_data_path\"])\n",
        "  # 평가 데이터 전처리\n",
        "  preprocessed_test_datas = prepro(test_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  # tensorflow session 옵션 설정\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n",
        "    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n",
        "\n",
        "    # np.random.shuffle(preprocessed_test_datas)\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 분할하여 저장\n",
        "    batches = get_batch(preprocessed_test_datas, flags[\"batch_size\"])\n",
        "\n",
        "    # 전체 음절 수, 정답을 맞춘 음절 수\n",
        "    total_count, correct_count = 0, 0\n",
        "    for inputs, inputs_length, targets in tqdm(batches[:10]):\n",
        "      predict_op = sess.run(model.predict_op,\n",
        "                            feed_dict={ model.inputs:inputs, \n",
        "                                       model.inputs_length:inputs_length, \n",
        "                                       model.keep_prob:flags[\"keep_prob\"] }\n",
        "                           )\n",
        "      \n",
        "      # 모델의 outputs에는 각 클래스에 대한 분포가 저장되어 있고\n",
        "      # np.argmax 함수를 통하여 가장 확률이 높은 클래스를 선택\n",
        "      # 예시) \n",
        "      #  predict_op = [0,1, 0.3, 0.2] (각각 0일 확률, 1일 확률, 2일 확률)\n",
        "      #  np.argmax(predict_op) = 1\n",
        "      predict, correct = np.argmax(predict_op[0], axis=-1), targets[0]\n",
        "      \n",
        "      # padding 처리해준 부분 제거\n",
        "      predict, correct = np.trim_zeros(predict), np.trim_zeros(correct)\n",
        "      \n",
        "      # print([idx2eumjeol[temp] for temp in inputs[0]])\n",
        "      # print([idx2label[temp] for temp in predict])\n",
        "      # print([idx2label[temp] for temp in correct])\n",
        "\n",
        "      predict_sentence = make_sentence(inputs[0], predict, idx2eumjeol, idx2label)\n",
        "      correct_sentence = make_sentence(inputs[0], correct, idx2eumjeol, idx2label)\n",
        "      print(\"\\n정답 : \" + correct_sentence)\n",
        "      print(\"출력 : \" + predict_sentence + \"\\n\")\n",
        "      \n",
        "      correct_count += np.sum(np.equal(predict, correct))\n",
        "      total_count += inputs_length[0]\n",
        "\n",
        "    print(\"Accuracy : \" + str(100.0*correct_count/total_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b7ddMCzt8_",
        "colab_type": "text"
      },
      "source": [
        "<h2>모델의 hyper parameter 설정, 학습 및 평가 실행</h2>\n",
        "\n",
        "<pre>\n",
        "root_dir : 코드와 데이터가 있는 디렉토리 경로\n",
        "save_dir : 학습한 모델 파일을 저장할 디렉토리 경로(디렉토리가 존재하지 않을 경우 자동으로 생성)\n",
        "\n",
        "<b>flags : hyper parameter를 저장할 딕셔너리</b>\n",
        "  flags.mode = 학습 또는 평가 설정(\"train\" or \"test\")\n",
        "  flags.save_dir = 학습한 모델 파일을 저장할 디렉토리 경로\n",
        "  flags.batch_size = 한번에 학습할 데이터의 개수\n",
        "  flags.epoch = 학습 횟수\n",
        "  flags.learning_rate = 학습률\n",
        "  flags.keep_prob = 노드를 보전할 확률\n",
        "  flags.max_length = 음절 sequence의 최대 길이\n",
        "  flags.embedding_size = 음절 임베딩 사이즈\n",
        "  flags.hidden_size = rnn cell의 히든 사이즈\n",
        "  flags.encoder_vocab_size = 음절 어휘 딕셔너리의 사이즈\n",
        "  flags.label_vocab_size = 라벨 딕셔너리의 사이즈\n",
        "  flags.train_data_path = 학습데이터 파일 경로\n",
        "  flags.test_data_path = 평가데이터 파일 경로\n",
        "  flags.vocab_data_path = 음절 어휘 파일 경로\n",
        "\n",
        "<b>mode 별 hyper parameter 변경</b>\n",
        "  학습하는 경우 : mode를 \"train\"으로 설정, 나머지는 기본 설정 그대로 유지\n",
        "  평가하는 경우 : mode를 \"test\"로, batch_size는 1로, keep_prob은 1.0으로 변경\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQfWrdn33utZ",
        "colab_type": "code",
        "outputId": "042311b3-e246-44d5-b059-b89e5c6605a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_dir = \"/gdrive/My Drive/colab/week10\"\n",
        "  save_dir = os.path.join(root_dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"test\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":1,\n",
        "           \"epoch\":100,\n",
        "           \"learning_rate\":0.0005,\n",
        "           \"keep_prob\":1.0,\n",
        "           \"max_length\":100,\n",
        "           \"embedding_size\":100,\n",
        "           \"hidden_size\":100,\n",
        "           \"encoder_vocab_size\":2023,\n",
        "           \"label_vocab_size\":9,\n",
        "           \"train_data_path\":os.path.join(root_dir, \"ner_train.txt\"),\n",
        "           \"test_data_path\":os.path.join(root_dir, \"ner_test.txt\"),\n",
        "           \"eumjeol_vocab_data_path\":os.path.join(root_dir, \"vocab.txt\"),\n",
        "           \"label_vocab_data_path\":os.path.join(root_dir, \"label_vocab.txt\")\n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags)\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Read from : /gdrive/My Drive/colab/week10/model/model_100.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:00<00:02,  4.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "정답 : <근대:DT> 이후 <현대:DT>까지 내려오는 <서양:LC>의 人間觀은 물론 사람과 시대에 따라 여러 가지 있었지만 대체로 이 <르네상스:DT> 인간관을 그 基調로 하고 세워진 것이라고 볼 수 있다.\n",
            "출력 : 근대 이후 <현대:PS>까지 내려오는 서양의 人間觀은 물론 사람과 시대에 따라 여러 가지 있었지만 대체로 이 르네상스 인간관을 그 基調로 하고 세워진 것이라고 볼 수 있다.\n",
            "\n",
            "\n",
            "정답 : 어느 날 <현우:PS>는 뜰 끝에 나와 앉아서 동무한테서 빌린 중학 강의록 책을 보고 있었다.\n",
            "출력 : 어느 날 <현우:PS>는 뜰 끝에 나와 앉아서 동무한테서 빌린 중학 강의록 책을 보고 있었다.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [00:00<00:00,  5.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "정답 : 그는 멍청한 <우디:PS>를 부사장으로 승진시키고, 폭력배인 케빈 스페이시 일당을 통해 공장 설비를 파손시키고 그 책임을 물어 노동자들을 해고하려 한다.\n",
            "출력 : 그는 멍청한 우디를 부사장으로 승진시키고, 폭력배인 케빈 스페이시 일당을 통해 공장 설비를 파손시키고 그 책임을 물어 노동자들을 해고하려 한다.\n",
            "\n",
            "\n",
            "정답 : 누이동생의 말에 부시시 일어난 <수창이:PS> 아버지는 크게 하품을 하고 나서 힘없이 말한다.\n",
            "출력 : 누이동생의 말에 부시시 일어난 <수창이:PS> 아버지는 크게 하품을 하고 나서 힘없이 말한다.\n",
            "\n",
            "\n",
            "정답 : 그리고 물질과 그것의 운동, 이 두 가지 요소가 바로 <데카르트:PS>의 기계적 철학의 근본요소가 되었다.\n",
            "출력 : 그리고 물질과 그것의 운동, 이 두 가지 요소가 바로 <데카르트:PS>의 기계적 철학의 근본요소가 되었다.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [00:00<00:00,  7.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "정답 : <탁:PS>선생의 말이 알듯 모를 듯 싶었지만 좌우지간 <영애:PS>는 가수만 되면 되는 거였다.\n",
            "출력 : <탁:PS>선생의 말이 알듯 모를 듯 싶었지만 좌우지간 <영애:PS>는 가수만 되면 되는 거였다.\n",
            "\n",
            "\n",
            "정답 : <박:PS>기자 옆에 누워서 아삭아삭 건빵을 씹어먹던 <김:PS>일병이 빨딱 일어나 앉았습니다.\n",
            "출력 : 박기자 옆에 누워서 아삭아삭 건빵을 씹어먹던 <김일:PS>병이 빨딱 일어나 앉았습니다.\n",
            "\n",
            "\n",
            "정답 : <鄭夢周:PS>를 때려 죽였는데도 불구하고 아직도 <善竹橋:LC>에 피가 흐른다는 것은 무엇인가?\n",
            "출력 : <鄭夢周:PS>를 때려 죽였는데도 불구하고 아직도 善竹橋에 피가 흐른다는 것은 무엇인가?\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  7.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "정답 : 따라서 <일년:DT>이 비록 넉넉하지는 못할지라도 반드시 무슨 초월적인 힘에 의지해야만 그가 원하는 바를 알 수 있게 될 만큼의 짧은 기간 또한 아니었다.\n",
            "출력 : 따라서 일년이 비록 넉넉하지는 못할지라도 반드시 무슨 초월적인 힘에 의지해야만 그가 원하는 바를 알 수 있게 될 만큼의 짧은 기간 또한 아니었다.\n",
            "\n",
            "\n",
            "정답 : <정부:OG>는 잠수함 침투사건을 계기로 <북한:LC>의 실체를 새롭게 인식하고 <북한:LC>에 대해 더 이상 양보가 없는 강경한 자세로 전환하고 있다.\n",
            "출력 : <정부:PS>는 잠수함 침투사건을 계기로 북한의 실체를 새롭게 인식하고 <북한:PS>에 대해 더 이상 양보가 없는 강경한 자세로 전환하고 있다.\n",
            "\n",
            "Accuracy : 96.33943427620632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}