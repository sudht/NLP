{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week7_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudht/NLP/blob/master/week7_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9",
        "colab_type": "text"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-lNQfT1D1v",
        "colab_type": "code",
        "outputId": "bbdfd6b6-2d33-4ec9-ae53-f27d7601b76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWZLECjoJkn",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델</h2>\n",
        "\n",
        "![실습 그림](http://nlp.kangwon.ac.kr/~nlpdemo/rnn_lstm.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5861idd25QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "class RNN(object):\n",
        "  \n",
        "  def __init__(self, flags):\n",
        "    self.encoder_vocab_size = flags[\"encoder_vocab_size\"]    # 음절 개수\n",
        "    self.label_vocab_size = flags[\"label_vocab_size\"]\n",
        "    self.embedding_size = flags[\"embedding_size\"]            # 음절 임베딩 사이즈\n",
        "    self.hidden_size = flags[\"hidden_size\"]                  # GRU 히든 사이즈\n",
        "\n",
        "    self.max_length = flags[\"max_length\"]                    # 음절 최대 길이\n",
        "    self.keep_prob = flags[\"keep_prob\"]                      # 노드를 보전할 확률\n",
        "    self.learning_rate = flags[\"learning_rate\"]              # 학습률\n",
        "    self.mode = flags[\"mode\"]                                # 학습 or 평가 상태\n",
        "\n",
        "    # build graph\n",
        "    self._input_init()\n",
        "    self._embedding_init()\n",
        "    self._rnn_init()\n",
        "    self._train_init()\n",
        "    self._predict_init()\n",
        "        \n",
        "  # 입력 데이터, 입력 데이터의 길이, 정답 데이터, keep_prob 값을 담을 tensor 선언  \n",
        "  def _input_init(self):\n",
        "    # 입력 데이터\n",
        "    self.inputs = tf.placeholder(tf.int32, [None, self.max_length], name=\"inputs\")\n",
        "    \n",
        "    # 입력 데이터의 길이\n",
        "    self.inputs_length = tf.placeholder(tf.int32, [None], name=\"inputs_length\")\n",
        "\n",
        "    # 정답 데이터\n",
        "    self.targets = tf.placeholder(tf.int32, [None, self.max_length], name=\"targets\")\n",
        "    # loss를 구할 때 각 sequence의 가중치를 설정\n",
        "    self.target_weights = tf.sequence_mask(lengths=self.inputs_length, maxlen=self.max_length, dtype=tf.float32, name='weight')\n",
        "    # 노드를 보전할 확률\n",
        "    self.keep_prob = tf.placeholder(tf.float32, [], \"keep_prob\")\n",
        "\n",
        "  def _embedding_init(self):\n",
        "    # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 tensor\n",
        "    # 기존에 사전학습 된 음절 임베딩을 사용할 수도 있고 랜덤으로 초기화 한 후,\n",
        "    # 모델 학습 과정 중에 같이 학습 시키는 것도 가능\n",
        "    # 예제 코드는 랜덤으로 초기화 한 후 같이 학습하도록 설정\n",
        "    self.encoder_embeddings = tf.get_variable(\"encoder_embedding\",\n",
        "                                             shape = [self.encoder_vocab_size, self.embedding_size],\n",
        "                                             dtype = tf.float32, trainable=True,\n",
        "                                             initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "\n",
        "  def _rnn_init(self):\n",
        "    with tf.name_scope(\"rnn_layer\"):\n",
        "      # rnn에서 사용할 cell 설정\n",
        "      # 예제 코드에서는 bidirectional lstm를 사용하기 때문에 정방향 cell과 역방향 cell 두개를 사용\n",
        "      fw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      bw_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, input_keep_prob=self.keep_prob, output_keep_prob=self.keep_prob)\n",
        "      \n",
        "      # 임베딩 tensor를 이용하여 입력데이터의 각 음절 index를 대응하는 임베딩 벡터로 치환\n",
        "      # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "      self.lookup_inputs = tf.nn.embedding_lookup(self.encoder_embeddings, self.inputs)\n",
        "\n",
        "      # tf.nn.bidirectional_dynamic_rnn 라이브러리를 사용하여 rnn layer의 출력을 구한다\n",
        "      # outputs : 각 step의 rnn 출력값, state : 마지막 hidden state 값\n",
        "      (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, self.lookup_inputs,\n",
        "                                                                                       sequence_length=self.inputs_length,\n",
        "                                                                                       dtype=tf.float32,\n",
        "                                                                                       time_major=False)\n",
        "\n",
        "      # 예제 코드에서는 각 step의 출력값을 사용한다.\n",
        "      # 양방향으로 생성된 rnn 결과를 연결하여 하나의 벡터로 표현\n",
        "      # (batch_size, max_length, hidden_size)*2 -> (batch_size, max_length, hidden_size*2)\n",
        "      outputs = tf.concat([fw_outputs, bw_outputs], axis=2)\n",
        "      \n",
        "      # fully_connected layer를 통하여 출력 크기를 label_vocab_size에 맞춰줌\n",
        "      # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, label_vocab_size)\n",
        "      self.outputs = tf.layers.dense(inputs=outputs,\n",
        "                                     units=self.label_vocab_size,\n",
        "                                     activation=tf.nn.relu,\n",
        "                                     use_bias=False,\n",
        "                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "    \n",
        "    \n",
        "  def _predict_init(self):\n",
        "    # 각 라벨의 분포 값들을 softmax 함수를 이용하여 0~1사이의 값으로 변경\n",
        "    self.predict_op = tf.nn.softmax(logits=self.outputs, axis=-1)\n",
        "\n",
        "  def _train_init(self):\n",
        "    if self.mode == \"train\":\n",
        "      with tf.variable_scope(\"train_layer\"):\n",
        "        # 모델의 출력인 self.outputs 와 self.targets를 비교하여 loss 계산\n",
        "        self.loss = tf.contrib.seq2seq.sequence_loss(self.outputs, self.targets, self.target_weights)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.grads = self.optimizer.compute_gradients(self.loss)\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwrnE5li9bmF",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 읽고 전처리 하기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_file(file_path)</b>\n",
        "  \"train_file.txt\", \"test_file.txt\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  데이터 예시)\n",
        "    나 의 사 랑 한 글 날 \\t B I B I B I I\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    \n",
        "    출력 예시)\n",
        "      [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\"], 7, [\"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\"]\n",
        "      \n",
        "<b>2. read_vocab_file(file_path)</b>\n",
        "  \"vocab.txt\" 파일을 읽고 음절과 라벨을 indexing하기 위한 딕셔너리를 생성\n",
        "   \n",
        "  read_vocab_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    eumjeol2idx, idx2eumjeol, label2idx, idx2label 딕셔너리\n",
        "    \n",
        "    eumjeol2idx : 음절을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환하기 위한 딕셔너리\n",
        "    label2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환하기 위한 딕셔너리\n",
        "    \n",
        "<b>3. prepro(datas, eumjeol2idx, label2idx)</b>\n",
        "  입력 데이터의 각 음절과 라벨을 indexing, 음절 sequence의 길이를 고정된 길이로 변환\n",
        "   \n",
        "  prepro(datas, eumjeol2idx, label2idx)\n",
        "  args\n",
        "    file_path : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "  return\n",
        "    입력 데이터의 각 음절을 indexing하고 라벨을 one_hot 인코딩한 리스트\n",
        "    \n",
        "  전처리 예시)\n",
        "    [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\"], 7, [\"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\"]\n",
        "    \n",
        "    사전 설정한 문장의 최대 길이를 10이라고 가정\n",
        "    \n",
        "    [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\"] -> [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\", \"<PAD>\", \"<PAD>\", \"<PAD>\"]\n",
        "    [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\", \"<PAD>\", \"<PAD>\", \"<PAD>\"] -> [ 23, 2, 55, 65, 96, 12, 4, 0, 0, 0 ]\n",
        "    \n",
        "    [ \"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\" ] -> [ \"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\", \"<PAD>\", \"<PAD>\", \"<PAD>\" ]\n",
        "    [ \"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\", \"<PAD>\", \"<PAD>\", \"<PAD>\" ] -> [ 1, 2, 1, 2, 1, 2, 2, 0, 0, 0 ]    \n",
        " </pre>\n",
        "        \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hopD20S9VJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 데이터를 읽고 음절 sequence와 라벨 sequence를 분리\n",
        "# 음절 sequence의 길이를 구하고 음절 sequence, 음절 sequence의 길이, 라벨 sequence를 datas 리스트에 저장\n",
        "def read_file(file_path):\n",
        "  with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "    lines = inFile.readlines()\n",
        "  datas = []\n",
        "  for line in lines:\n",
        "    # 입력 문장을 \\t으로 분리\n",
        "    pieces = line.strip().split(\"\\t\")\n",
        "    eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n",
        "    eumjeol_length = len(eumjeol_sequence)\n",
        "\n",
        "    datas.append((eumjeol_sequence, eumjeol_length, label_sequence))\n",
        "\n",
        "  return datas\n",
        "  \n",
        "def read_vocab_file(file_path):\n",
        "  eumjeol2idx = {\"<PAD>\":0}\n",
        "  idx2eumjeol = {0:\"<PAD>\"}\n",
        "  label2idx = {\"<PAD>\":0, \"B\":1, \"I\":2}\n",
        "  idx2label = {0:\"<PAD>\", 1:\"B\", 2:\"I\"}\n",
        "  \n",
        "  with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "    lines = inFile.readlines()\n",
        "\n",
        "  count = 0\n",
        "  for line in lines:\n",
        "    # P.1\n",
        "    # 파일을 읽고 파일에 저장된 단어를 dictionary에 저장하는 코드\n",
        "    ############################################################\n",
        "    eumjeol = line.strip()\n",
        "    count = count + 1\n",
        "    \n",
        "    eumjeol2idx[eumjeol] = count\n",
        "    idx2eumjeol[count] = eumjeol\n",
        "    ############################################################\n",
        "  return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n",
        "        \n",
        "# 입력 데이터 전처리 \n",
        "def prepro(datas, eumjeol2idx, label2idx, max_length):\n",
        "  \n",
        "  preprocessed_datas = []\n",
        "  for eumjeol_sequence, eumjeol_length, label_sequence in datas:\n",
        "    \n",
        "    # max_length의 길이를 가지는 numpy array 생성하고 0으로 초기화\n",
        "    indexing_eumjeol_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    indexing_label_sequence = np.zeros(max_length, dtype=np.int32)\n",
        "    \n",
        "    # 각 음절과 라벨 index를 numpy array에 입력\n",
        "    \n",
        "    # P.2\n",
        "    # 입력 문장과 라벨을 숫자로 치환하는 코드\n",
        "    ############################################################\n",
        "    for index in range(eumjeol_length):\n",
        "      indexing_eumjeol_sequence[index] = eumjeol2idx[eumjeol_sequence[index]]\n",
        "      indexing_label_sequence[index] = label2idx[label_sequence[index]]\n",
        "    ############################################################\n",
        "    \n",
        "      \n",
        "    preprocessed_datas.append((indexing_eumjeol_sequence, eumjeol_length, indexing_label_sequence))\n",
        "\n",
        "  return preprocessed_datas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTgxrex_qW--",
        "colab_type": "text"
      },
      "source": [
        "<h2>데이터 batch 단위로 나누기</h2>\n",
        "\n",
        "<pre>\n",
        "<b>get_batch(datas, batch_size)</b>\n",
        "  전체 데이터를 batch 단위로 나누어 주기 위한 함수\n",
        "  \n",
        "  get_batch(datas, batch_size)\n",
        "  args\n",
        "    datas : 음절 sequence, 음절 sequence의 길이, 각 음절에 대응하는 라벨 sequence를 담고 있는 리스트\n",
        "    batch_size : 한번에 학습할 데이터의 개수\n",
        "  return\n",
        "    batch 단위로 나뉘어진 데이터 리스트\n",
        "    \n",
        "  예시) \n",
        "    batch_size = 3 인 경우\n",
        "  \n",
        "    total_datas = [ (음절 sequence1, sequence1의 길이, 라벨 sequence1), ... , ((음절 sequence100, sequence100의 길이, 라벨 sequence100)) ]\n",
        "    \n",
        "    batches = [\n",
        "    [ [ 음절 sequence1, 음절 sequence2, 음절 sequence3 ], \n",
        "      [ sequence1의 길이, sequence2의 길이, sequence3의 길이 ],\n",
        "      [ 라벨 sequence1, 라벨 sequence2, 라벨 sequence3 ]\n",
        "    ],\n",
        "    \n",
        "    [ [ 음절 sequence4, 음절 sequence5, 음절 sequence6 ], \n",
        "      [ sequence4의 길이, sequence5의 길이, sequence6의 길이 ],\n",
        "      [ 라벨 sequence4, 라벨 sequence5, 라벨 sequence6 ]\n",
        "    ],\n",
        "    \n",
        "    ...\n",
        "    \n",
        "    ]\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG3GWKrAqQzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터를 batch 단위로 분할하여 저장\n",
        "def get_batch(datas, batch_size):\n",
        "  # batches : batch 단위로 저장할 리스트\n",
        "  # inputs : 각 batch 단위 별 음절 sequence를 저장할 리스트\n",
        "  # inputs_length : 각 batch 단위 별 음절 sequence 길이를 저장할 리스트\n",
        "  # targets : 각 batch 단위 별 라벨 sequence를 저장할 리스트\n",
        "  batches, inputs, inputs_length, targets = [], [], [], []\n",
        "  # P.3\n",
        "  # datas를 batch 크기로 잘라서 저장 및 return하는 함수\n",
        "  ############################################################\n",
        "  for indexing_eumjeol_sequence, eumjeol__length, indexing_label_sequence in datas:\n",
        "    inputs.append(indexing_eumjeol_sequence)\n",
        "    inputs_length.append(eumjeol__length)\n",
        "    targets.append(indexing_label_sequence)\n",
        "    \n",
        "    if(len(inputs) == batch_size):\n",
        "      batches.append((inputs, inputs_length, targets))\n",
        "      inputs, inputs_length, targets = [], [], []\n",
        "    ############################################################ \n",
        "\n",
        "  return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4uOGj7vwFFY",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(file_path) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 학습 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>3. epoch를 돌때마다 학습 데이터 셔플</b>\n",
        "\n",
        "<b>3. batch 단위로 학습을 수행</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPLgRpO9SAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags[\"vocab_data_path\"])\n",
        "  # 학습 데이터 읽기\n",
        "  train_datas = read_file(flags[\"train_data_path\"])\n",
        "  # 학습 데이터 전처리\n",
        "  preprocessed_train_datas = prepro(train_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  \n",
        "  # tensorflow session 옵션 설정\n",
        "  # allow_soft_placement=True : 어떤 device를 사용하여 연산할지 명시하지 않은 경우 자동으로 존재하는 디바이스 중에서 하나를 선택\n",
        "  # gpu_options=tf.GPUOptions(allow_growth=True) : 연산 실행 과정에서 필요한만큼의 gpu 메모리만 사용\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    for epoch in tqdm(range(flags[\"epoch\"])):\n",
        "      # 학습 데이터 셔플\n",
        "      np.random.shuffle(preprocessed_train_datas)\n",
        "      # 학습 데이터를 batch 단위로 분할하여 저장\n",
        "      batches = get_batch(preprocessed_train_datas, flags[\"batch_size\"])\n",
        "\n",
        "      losses = []\n",
        "      # batch 단위로 학습을 진행하며 각 batch 별 loss를 구한다\n",
        "      # batch 별 loss들의 평균을 구하여 이를 전체 데이터에 대한 loss로 사용\n",
        "      for inputs, inputs_length, targets in batches:\n",
        "        loss, train_op = sess.run([model.loss, model.train_op],\n",
        "                                  feed_dict={ model.inputs:inputs, \n",
        "                                             model.inputs_length:inputs_length, \n",
        "                                             model.targets:targets, \n",
        "                                             model.keep_prob:flags[\"keep_prob\"] }\n",
        "                                 )\n",
        "\n",
        "        losses.append(loss)\n",
        "        \n",
        "      # 학습한 모델 파일 저장\n",
        "      filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n",
        "      saver.save(sess, filename)\n",
        "\n",
        "      print(\"\\tEpoch : {}, Average_Loss : {}\".format(epoch+1, np.mean(losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wzroGrw2xm",
        "colab_type": "text"
      },
      "source": [
        "<h2>RNN 모델 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_vocab_file(file_path) 함수를 사용하여 딕셔너리 생성</b>\n",
        "\n",
        "<b>2. read_file(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>3. prepro 함수를 사용하여 평가 데이터 전처리</b>\n",
        "\n",
        "<b>4. RNN 모델 객체 선언</b>\n",
        "\n",
        "<b>5. tf.train.Saver() 객체를 사용하여 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴</b>\n",
        "\n",
        "<b>6. make_sentence(inputs, predict, correct, idx2eumjeol, idx2label) 함수를 이용하여 정답과 모델 출력 비교</b>\n",
        "  \n",
        "  make_sentence(inputs, predict, correct, idx2eumjeol, idx2label)\n",
        "  args\n",
        "    inputs : 음절 sequence\n",
        "    predict : 모델 출력 라벨 sequence\n",
        "    correct : 정답 라벨 sequence\n",
        "    idx2eumjeol : index를 대응하는 음절로 치환해주는 딕셔너리\n",
        "    idx2label : index를 대응하는 라벨로 치환해주는 딕셔너리\n",
        "  return\n",
        "    정답 문장과 모델 출력 문장\n",
        "  \n",
        "  예시)\n",
        "    inputs = [\"나\", \"의\", \"사\", \"랑\", \"한\", \"글\", \"날\"]\n",
        "    predict = [\"B\", \"B\", \"B\", \"I\", \"I\", \"I\", \"I\"]\n",
        "    correct = [\"B\", \"I\", \"B\", \"I\", \"B\", \"I\", \"I\"]\n",
        "    \n",
        "    정답 문장 : 나의 사랑 한글날\n",
        "    모델 출력 문장 : 나 의 사랑한글날\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPLfTilw9MVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm    \n",
        "\n",
        "# 모델 출력 라벨 sequence와 정답 라벨 sequence를 기반으로\n",
        "# 모델 출력 문장과 정답 문장 출력\n",
        "def make_sentence(inputs, predict, correct, idx2eumjeol, idx2label):\n",
        "  # 빈 문자열 생성\n",
        "  predict_sentence, correct_sentence = \"\", \"\"\n",
        "  \n",
        "    # P.4\n",
        "    # 정답 문장과 모델이 예측한 답변을 return 하는 코드\n",
        "    ############################################################\n",
        "  for idx in range(len(inputs)):\n",
        "    eumjeol = idx2eumjeol[inputs[idx]]\n",
        "    predict_label = idx2label[predict[idx]]\n",
        "    correct_label = idx2label[correct[idx]]\n",
        "    \n",
        "    if(correct_label == \"<PAD>\"):\n",
        "      break\n",
        "      \n",
        "    if(idx == 0):\n",
        "      predict_sentence += eumjeol\n",
        "      correct_sentence += eumjeol\n",
        "      continue\n",
        "      \n",
        "    if(predict_label == \"B\"):\n",
        "      predict_sentence += \" \"\n",
        "    if(correct_label == 'B'):\n",
        "      correct_sentence += \" \"\n",
        "      \n",
        "    predict_sentence += eumjeol\n",
        "    correct_sentence += eumjeol\n",
        "    ############################################################\n",
        "    \n",
        "  return predict_sentence, correct_sentence\n",
        "      \n",
        "\n",
        "def test(flags):\n",
        "  # vocab.txt 파일을 읽고 딕셔너리 생성\n",
        "  eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_file(flags[\"vocab_data_path\"])\n",
        "  # 평가 데이터 읽기\n",
        "  test_datas = read_file(flags[\"test_data_path\"])\n",
        "  # 평가 데이터 전처리\n",
        "  preprocessed_test_datas = prepro(test_datas, eumjeol2idx, label2idx, flags[\"max_length\"])\n",
        "  \n",
        "  # 모델 객체 선언\n",
        "  model = RNN(flags)\n",
        "  # tensorflow session 옵션 설정\n",
        "  sess_config = tf.ConfigProto(allow_soft_placement=True, \n",
        "                               gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "  # tensorflow를 실행하기 위한 session\n",
        "  with tf.Session(config=sess_config) as sess:\n",
        "    # 그래프 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n",
        "    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 분할하여 저장\n",
        "    batches = get_batch(preprocessed_test_datas, flags[\"batch_size\"])\n",
        "\n",
        "    # 전체 음절 수, 정답을 맞춘 음절 수\n",
        "    total_count, correct_count = 0, 0\n",
        "    for inputs, inputs_length, targets in tqdm(batches):\n",
        "      predict_op = sess.run(model.predict_op,\n",
        "                            feed_dict={ model.inputs:inputs, \n",
        "                                       model.inputs_length:inputs_length, \n",
        "                                       model.keep_prob:flags[\"keep_prob\"] }\n",
        "                           )\n",
        "      \n",
        "      # 모델의 outputs에는 각 클래스에 대한 분포가 저장되어 있고\n",
        "      # np.argmax 함수를 통하여 가장 확률이 높은 클래스를 선택\n",
        "      # 예시) \n",
        "      #  predict_op = [0,1, 0.3, 0.2] (각각 0일 확률, 1일 확률, 2일 확률)\n",
        "      #  np.argmax(predict_op) = 1\n",
        "      predict, correct = np.argmax(predict_op[0], axis=-1), targets[0]\n",
        "      \n",
        "      # padding 처리해준 부분 제거\n",
        "      predict, correct = np.trim_zeros(predict), np.trim_zeros(correct)\n",
        "      \n",
        "      predict_sentence, correct_sentence = make_sentence(inputs[0], predict, correct, idx2eumjeol, idx2label)\n",
        "      print(\"\\n정답 : \" + correct_sentence)\n",
        "      print(\"모델 출력 : \" + predict_sentence)\n",
        "      print()\n",
        "      \n",
        "      correct_count += np.sum(np.equal(predict, correct))\n",
        "      total_count += inputs_length[0]\n",
        "\n",
        "    print(\"\\tAccuracy : \" + str(100.0*correct_count/total_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b7ddMCzt8_",
        "colab_type": "text"
      },
      "source": [
        "<h2>모델의 hyper parameter 설정, 학습 및 평가 실행</h2>\n",
        "\n",
        "<pre>\n",
        "root_dir : 코드와 데이터가 있는 디렉토리 경로\n",
        "save_dir : 학습한 모델 파일을 저장할 디렉토리 경로(디렉토리가 존재하지 않을 경우 자동으로 생성)\n",
        "\n",
        "<b>flags : hyper parameter를 저장할 딕셔너리</b>\n",
        "  flags.mode = 학습 또는 평가 설정(\"train\" or \"test\")\n",
        "  flags.save_dir = 학습한 모델 파일을 저장할 디렉토리 경로\n",
        "  flags.batch_size = 한번에 학습할 데이터의 개수\n",
        "  flags.epoch = 학습 횟수\n",
        "  flags.learning_rate = 학습률\n",
        "  flags.keep_prob = 노드를 보전할 확률\n",
        "  flags.max_length = 음절 sequence의 최대 길이\n",
        "  flags.embedding_size = 음절 임베딩 사이즈\n",
        "  flags.hidden_size = rnn cell의 히든 사이즈\n",
        "  flags.encoder_vocab_size = 음절 어휘 딕셔너리의 사이즈\n",
        "  flags.label_vocab_size = 라벨 딕셔너리의 사이즈\n",
        "  flags.train_data_path = 학습데이터 파일 경로\n",
        "  flags.test_data_path = 평가데이터 파일 경로\n",
        "  flags.vocab_data_path = 음절 어휘 파일 경로\n",
        "\n",
        "<b>mode 별 hyper parameter 변경</b>\n",
        "  학습하는 경우 : mode를 \"train\"으로 설정, 나머지는 기본 설정 그대로 유지\n",
        "  평가하는 경우 : mode를 \"test\"로, batch_size는 1로, keep_prob은 1.0으로 변경\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQfWrdn33utZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "d15ccb88-1120-4f2a-8a69-7bdb05ead3e5"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_dir = \"/gdrive/My Drive/colab/week7\"\n",
        "  save_dir = os.path.join(root_dir, \"model\")\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  flags = {\"mode\":\"train\",\n",
        "           \"save_dir\":save_dir,\n",
        "           \"batch_size\":64,\n",
        "           \"epoch\":20,\n",
        "           \"learning_rate\":0.001,\n",
        "           \"keep_prob\":0.7,\n",
        "           \"max_length\":100,\n",
        "           \"embedding_size\":100,\n",
        "           \"hidden_size\":100,\n",
        "           \"encoder_vocab_size\":2457,\n",
        "           \"label_vocab_size\":3,\n",
        "           \"train_data_path\":os.path.join(root_dir, \"train_file.txt\"),\n",
        "           \"test_data_path\":os.path.join(root_dir, \"test_file.txt\"),\n",
        "           \"vocab_data_path\":os.path.join(root_dir, \"vocab.txt\")\n",
        "          }\n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  if(flags[\"mode\"] == \"train\"):\n",
        "      train(flags)\n",
        "  elif(flags[\"mode\"] == \"test\"):\n",
        "      flags[\"batch_size\"] = 1  \n",
        "      flags[\"keep_prob\"] = 1.0\n",
        "      test(flags)\n",
        "  else:\n",
        "      print(\"Unknown mode\")\n",
        "      exit(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [00:06<02:11,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 1, Average_Loss : 1.0525599718093872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 2/20 [00:12<01:56,  6.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 2, Average_Loss : 0.7576602101325989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 3/20 [00:17<01:45,  6.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 3, Average_Loss : 0.6471309065818787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 4/20 [00:23<01:35,  5.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 4, Average_Loss : 0.6232826113700867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 5/20 [00:28<01:26,  5.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 5, Average_Loss : 0.6145946383476257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 6/20 [00:33<01:18,  5.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 6, Average_Loss : 0.6047953367233276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 7/20 [00:39<01:12,  5.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 7, Average_Loss : 0.5858564376831055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 8/20 [00:44<01:05,  5.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 8, Average_Loss : 0.5427664518356323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 9/20 [00:49<00:59,  5.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 9, Average_Loss : 0.47025471925735474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 10/20 [00:55<00:53,  5.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 10, Average_Loss : 0.3948332965373993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 55%|█████▌    | 11/20 [01:00<00:48,  5.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 11, Average_Loss : 0.3507411479949951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 12/20 [01:05<00:42,  5.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 12, Average_Loss : 0.3292023241519928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 65%|██████▌   | 13/20 [01:11<00:37,  5.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 13, Average_Loss : 0.31036314368247986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 14/20 [01:16<00:32,  5.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 14, Average_Loss : 0.29846176505088806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 15/20 [01:21<00:26,  5.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 15, Average_Loss : 0.28136444091796875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 16/20 [01:27<00:21,  5.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 16, Average_Loss : 0.2686980068683624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 85%|████████▌ | 17/20 [01:32<00:16,  5.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 17, Average_Loss : 0.25783881545066833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 18/20 [01:37<00:10,  5.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 18, Average_Loss : 0.2506544888019562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 95%|█████████▌| 19/20 [01:43<00:05,  5.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 19, Average_Loss : 0.24192465841770172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 20/20 [01:48<00:00,  5.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch : 20, Average_Loss : 0.23548480868339539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhYWAW6BPNO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}